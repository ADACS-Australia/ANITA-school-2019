{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANITA astroinformatics summer school 2019 - \"Rise of the machines\"\n",
    "\n",
    "## Part I - data cleaning and exploration\n",
    "\n",
    "This lesson is adapted from the [Data Carpentry Ecology lesson](http://www.datacarpentry.org/python-ecology-lesson/)\n",
    "\n",
    "We'll be using a gitter chat room to ask questions and chat:\n",
    "https://gitter.im/ANITA-astroinformatics-school/2019-school\n",
    "\n",
    "After finishing the course an updated notebook with the answers will be made available in the github repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Pandas and DataFrames](#Pandas-and-DataFrames)\n",
    "2. [Dealing With Data In Pandas](#Dealing-With-Data-In-Pandas)\n",
    "3. [Calculating statistics in a pandas DataFrame](#Calculating-statistics-in-a-pandas-DataFrame)\n",
    "6. [Indexing & Slicing in Pandas](#Indexing-&-Slicing-in-Pandas)\n",
    "5. [Cleaning our data for machine learning](#Cleaning-our-data-for-machine-learning)\n",
    "4. [Exploratory analysis](#Exploratory-analysis)\n",
    "7. [Supplement: Merging DataFrames](#Supplement:-Merging-DataFrames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required libraries\n",
    "\n",
    "This notebook uses several Python packages that come standard with the [Anaconda Python distribution](http://continuum.io/downloads). The primary libraries that we'll be using are:\n",
    "\n",
    "* **pandas**: a DataFrame structure to store data in memory and work with it easily and efficiently.\n",
    "* **seaborn**: a advanced statistical plotting library.\n",
    "\n",
    "To make sure you have all of the packages you need, install them with `conda`:\n",
    "\n",
    "    conda install pandas seaborn\n",
    "\n",
    "`conda` may ask you to update some of the packages if you don't have the most recent version. Allow it to do so.\n",
    "\n",
    "Alternatively, if you can install the packages with [pip](https://pip.pypa.io/en/stable/installing/) (a Python package manager):\n",
    "\n",
    "    pip install pandas seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas and DataFrames\n",
    "\n",
    "\n",
    "One of the best options for working with tabular data in Python is to use the\n",
    "[Python Data Analysis Library](http://pandas.pydata.org/) (a.k.a. Pandas). The\n",
    "Pandas library provides data structures, produces high quality plots with\n",
    "[matplotlib](http://matplotlib.org/) and integrates nicely with other libraries\n",
    "that use [NumPy](http://www.numpy.org/) (which is another Python library) arrays.\n",
    "\n",
    "Each time we call a function that's in a library, we use the syntax\n",
    "`LibraryName.FunctionName`. Adding the library name with a `.` before the\n",
    "function name tells Python where to find the function. \n",
    "\n",
    "We will import Pandas as `pd`, a common abbreviation of this library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So What's a DataFrame?\n",
    "\n",
    "A DataFrame is a 2-dimensional (labeled) data structure that stores data of different\n",
    "types (including characters, integers, floating point values, factors and more) in columns. \n",
    "\n",
    "It is similar to a spreadsheet or an SQL table or the `data.frame` in R. \n",
    "\n",
    "A DataFrame always has an index (0-based) and works best with *tidy data*.\n",
    "According to [Hadley Wickham](http://vita.had.co.nz/papers/tidy-data.html):\n",
    "> Tidy datasets are easy to manipulate, model and visualize, and have a specific structure: each variable is a column, each observation is a row, and each type of observational unit is a table. \n",
    "\n",
    "More info on the pandas DataFrame: https://pandas.pydata.org/pandas-docs/stable/dsintro.html#dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data into a pandas DataFrame\n",
    "\n",
    "### Our Data \n",
    "\n",
    "For this lesson, we will be using [Galaxy Zoo DR1 data](https://www.google.com/search?q=galaxy+zoo&ie=utf-8&oe=utf-8&client=firefox-b-ab). Galaxy Zoo is described in Lintott et al. 2008, MNRAS, 389, 1179 and the data release is described in Lintott et al. 2011, 410, 166.\n",
    "\n",
    "The table we use is an adapted version of Table 2, listing classifications of galaxies which have spectra included in SDSS Data Release 7. The debiased fraction of the votes in elliptical and spiral categories is given, along with flags identifying systems as classified as spiral, elliptical or uncertain.\n",
    "\n",
    "\n",
    "| Column           | Description                             |\n",
    "|------------------|-----------------------------------------|\n",
    "| id               | SDSS ID, objects taken from DR7         |\n",
    "| ra               | Right Ascension  (HMS)                  |\n",
    "| dec              | Declination (DMS)                       |\n",
    "| nvote            | number of votes                         |\n",
    "| p_e              | debiased vote fraction Ellipticals      |\n",
    "| p_s              | debiased vote fraction all Spirals      |\n",
    "| type             | whether final vote is E, S or U         |\n",
    "| class            | spiral or elliptical class, eg E0 or CW |\n",
    "\n",
    "Galaxies flagged as ‘elliptical’ or ‘spiral’ require 80 per cent of the vote in that category after the debiasing procedure has been applied; all other galaxies are flagged ‘uncertain’.\n",
    "Note, the elliptical class is randomly assigned. The spiral class is based on the highest vote fraction of the spiral classes in the Galaxy Zoo DR 1 Table 2 data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting in the same spot\n",
    "\n",
    "To help the lesson run smoothly, let's ensure everyone is in the same directory.\n",
    "This should help us avoid path and file name issues. \n",
    "\n",
    "At this time please navigate to the workshop directory (ie, the directory with the notebooks in them). \n",
    "If you working in IPython Notebook be sure that you start your notebook in the workshop directory.\n",
    "\n",
    "\n",
    "A quick aside, there are Python libraries like [OS\n",
    "Library](https://docs.python.org/3/library/os.html) that can work with our\n",
    "directory structure, however, that is not our focus today.\n",
    "\n",
    "\n",
    "If you **need to change your directory** ```import os``` and use ```os.chdir```\n",
    "\n",
    "Or you can use **%** to access the command line, e.g. ```% cd folder_name```\n",
    "\n",
    "--- \n",
    "\n",
    "We will begin by locating and reading our Galaxy Zoo data which is in a CSV file.\n",
    "We can use Pandas' `read_csv` function to pull the file directly into a\n",
    "[DataFrame](http://pandas.pydata.org/pandas-docs/stable/dsintro.html#dataframe).\n",
    "\n",
    "pandas has easy to use functions to read in data from various data sources, including CSV and HDF5, see [here](http://pandas.pydata.org/pandas-docs/version/0.24.0rc1/io.html#) for more info.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if you need to change your directory\n",
    "import os\n",
    "print(os.getcwd())\n",
    "\n",
    "#os.chdir(\"notebooks/\") #make sure you enter the correct fille path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#check your version, we need v0.19 or higher\n",
    "pd.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** you will need pandas version 0.19 or higher. Please update the package if required!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that pd.read_csv is used because we imported pandas as pd\n",
    "gal_df=pd.read_csv('data/GalaxyZoo1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the data type of the data stored in `gal_df` using the `type` method. \n",
    "\n",
    "Both the `type` method and `__class__` attribute tell us that `gal_df` is `<class 'pandas.core.frame.DataFrame'>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(gal_df)\n",
    "# or gal_df.__class__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the data types for each column in our DataFrame, we can use `gal_df.dtypes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gal_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas and base Python use slightly different names for data types. More on this\n",
    "is in the table below:\n",
    "\n",
    "| Pandas Type | Native Python Type | Description |\n",
    "|-------------|--------------------|-------------|\n",
    "| object | string | The most general dtype. Will be assigned to your column if column has mixed types (numbers and strings). |\n",
    "| int64  | int | Numeric characters. 64 refers to the memory allocated to hold this character. |\n",
    "| float64 | float | Numeric characters with decimals. If a column contains numbers and NaNs(see below), pandas will default to float64, in case your missing value has a decimal. |\n",
    "| datetime64, timedelta[ns] | N/A (but see the [datetime] module in Python's standard library) | Values meant to hold time data. Look into these for time series experiments. |\n",
    "\n",
    "[datetime]: http://doc.python.org/2/library/datetime.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There are multiple other methods that can be used to summarize and access the data stored in DataFrames**. \n",
    "\n",
    "Let's try out a few:\n",
    "\n",
    "### Your turn\n",
    "\n",
    "Try out the methods below to see what they return.\n",
    "\n",
    "1. `gal_df.columns`.\n",
    "2. `gal_df.head()`. Also, what does `gal_df.head(15)` do?\n",
    "3. `gal_df.tail()`.\n",
    "4. `gal_df.shape`. Take note of the output of the shape method. What format does it return the shape of the DataFrame in?\n",
    "\n",
    "HINT: [More on tuples, here](https://docs.python.org/3/tutorial/datastructures.html#tuples-and-sequences)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there were 667944 rows and 8 columns parsed.\n",
    "It looks like  the `read_csv` function in Pandas  read our file properly.\n",
    "\n",
    "Now we can start manipulating our data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing With Data In Pandas\n",
    "\n",
    "[[ go back to the top ]](#Table-of-Contents)\n",
    "\n",
    "## Selecting Data Using Labels (Column Headings)\n",
    "\n",
    "We use square brackets `[]` to select a subset of a Python object. For example,\n",
    "we can select all of the data from a column named `type` from the `gal_df`\n",
    "DataFrame by name:\n",
    "\n",
    "```python\n",
    "gal_df['type']\n",
    "# this syntax, calling the column as an attribute, gives you the same output\n",
    "gal_df.type\n",
    "```\n",
    "\n",
    "We can also create an new object that contains the data within the `type`\n",
    "column as follows:\n",
    "\n",
    "```python\n",
    "# create an object named gal_types that only contains the `types` column\n",
    "gal_types = gal_df['types']\n",
    "```\n",
    "\n",
    "We can pass a list of column names too, as an index to select columns in that\n",
    "order. This is useful when we need to reorganize our data.\n",
    "\n",
    "**NOTE:** If a column name is not contained in the DataFrame, an exception\n",
    "(error) will be raised.\n",
    "\n",
    "```python\n",
    "# select the type and class columns from the DataFrame\n",
    "gal_df[['type', 'class']]\n",
    "# what happens when you flip the order?\n",
    "gal_df[['class', 'type']]\n",
    "#what happens if you ask for a column that doesn't exist?\n",
    "gal_df['types']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Changing column types and content\n",
    "\n",
    "\n",
    "To modify the format of values within our data frame we can use the ```astype``` function (also remember in pandas the type is `float64`).\n",
    "\n",
    "Don't forget this is a function within pandas so we use it with a `.`, for example to convert \n",
    "the `nvote` field to floating point values we would run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the nvote field from an integer to a float\n",
    "gal_df['nvote']=gal_df['nvote'].astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gal_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we try to convert probability values to integers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gal_df['p_e']=gal_df['p_e'].astype('int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this throws a value error: `ValueError: Cannot convert non-finite values to integer`. \n",
    "\n",
    "If we look at the `p_e` column in our data we notice that\n",
    "there are NaN (**N**ot **a** **N**umber) values. *NaN* values are undefined\n",
    "values that cannot be represented mathematically. Pandas, for example, will read\n",
    "an empty cell in a CSV or Excel sheet as a NaN. NaNs have some desirable\n",
    "properties: if we were to average the `weight` column without replacing our NaNs,\n",
    "Python would know to skip over those cells.\n",
    "\n",
    "_Note: older pandas version do not know how to handle NaN, please update to v0.19_\n",
    "\n",
    "Check your pandas version using `pd.__version__`, if you need to update open a bash shell\n",
    "and type ```conda update pandas```, or if using pip ```pip install -U pandas```.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "Pandas generally vectorises operations, which means that looping through your data is not necessary.\n",
    "For example, we could perform a mathematical operation on an entire column of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gal_df.nvote.head())\n",
    "# multiply all votes by 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is equivalent to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Missing Data Values - NaN\n",
    "\n",
    "Dealing with missing data values is always a challenge. It's sometimes hard to\n",
    "know why values are missing - was it because of a data entry error? Or data that\n",
    "someone was unable to collect? Should the value be 0? We need to know how\n",
    "missing values are represented in the dataset in order to make good decisions.\n",
    "If we're lucky, we have some metadata that will tell us more about how null\n",
    "values were handled.\n",
    "\n",
    "For instance, in some disciplines, like astronomy or remote Sensing, missing data values are\n",
    "often defined as -9999. Having a bunch of -9999 values in your data could really\n",
    "alter numeric calculations. Often in spreadsheets, cells are left empty where no\n",
    "data are available. Pandas will, by default, replace those missing values with\n",
    "NaN. However, **it is good practice to get in the habit of intentionally marking\n",
    "cells that have no data, with a no data value!** That way there are no questions\n",
    "in the future when you (or someone else) explores your data.\n",
    "\n",
    "### Where Are the NaN's?\n",
    "\n",
    "Let's explore the NaN values in our data a bit further. \n",
    "First, let's figure out how many rows contain NaN values for the elliptical probabilities. \n",
    "We can do this by identifying how many rows have a NULL value (`.isnull`) or by counting the number of rows that have a meaningful value (e.g., p_e>0):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates a mask we can use to select the NULL values only. We'll talk more about [subsetting](#Indexing-&-Slicing-in-Pandas) data using masks later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of missing values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can replace all NaN values with zeroes using the `.fillna()` method (after\n",
    "making a copy of the data so we don't lose our work).\n",
    "\n",
    "However, NaN and 0 yield different analysis results. The mean value when NaN\n",
    "values are replaced with 0 is different from when NaN values are simply thrown\n",
    "out or ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace nan with 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check mean, how does it differ from before?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fill NaN values with any value that we choose. The code below fills all\n",
    "NaN values with a mean of all probability values.\n",
    "\n",
    "```python\n",
    " df1['p_e'] = gal_df['p_e'].fillna(gal_df['p_e'].mean())\n",
    "```\n",
    "\n",
    "**Note**: when we use the `.mean()` function, pandas automatically ignores the NaN.\n",
    "\n",
    "\n",
    "We could also chose to create a subset of our data, only keeping rows that do\n",
    "not contain NaN values, using `.dropna()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop NaN and compare mean before and after\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The point is to make conscious decisions about how to manage missing data.** \n",
    "This is where we think about how our data will be used and how these values will\n",
    "impact the scientific conclusions made from the data.\n",
    "\n",
    "Python gives us all of the tools that we need to account for these issues. We\n",
    "just need to be cautious about how the decisions that we make impact scientific\n",
    "results.\n",
    "\n",
    "\n",
    "**For more info on how to handle missing data in pandas, check out the [docs](http://pandas.pydata.org/pandas-docs/version/0.24.0rc1/missing_data.html)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating statistics in a Pandas DataFrame\n",
    "\n",
    "[[ go back to the top ]](#Table-of-Contents)\n",
    "\n",
    "We've read our data into Python. Next, let's perform some quick summary\n",
    "statistics to learn more about the data that we're working with. We might want\n",
    "to know how many galaxies of a specific type we have, or what the average number of votes was. \n",
    "We can perform summary stats quickly using groups. But first we need to figure out what we want to group by.\n",
    "\n",
    "Let's begin by exploring our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#column names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a list of all the galaxy types. The `pd.unique` function tells us all of\n",
    "the unique values in the `type` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful function is `.nunique()` which returns the number of unique entries in the specifed column(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics\n",
    "The Pandas function `.describe()` will return descriptive stats including: mean,\n",
    "median, max, min, std and count for a particular column in the data. Pandas'\n",
    "`describe` function will only return summary values for columns containing\n",
    "numeric data.\n",
    "We can calculate basic statistics for all numeric records in a single column or the entire dataframe using the\n",
    "syntax below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also extract one specific metric if we wish:\n",
    "\n",
    "```python\n",
    "gal_df['nvote'].min()\n",
    "gal_df['nvote'].max()\n",
    "gal_df['nvote'].mean()\n",
    "gal_df['nvote'].std()\n",
    "gal_df['nvote'].count()\n",
    "```\n",
    "\n",
    "## Groups in pandas and summary stats\n",
    "\n",
    "But if we want to summarize by one or more variables, for example galaxy type, we can\n",
    "use Pandas' `.groupby` method. Once we've created a groupby DataFrame, we\n",
    "can quickly calculate summary statistics by a group of our choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary statistics for all numeric columns by type\n",
    "grouped_data = gal_df.groupby('type')\n",
    "grouped_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# provide the mean for each numeric column by type\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `groupby` command is powerful in that it allows us to quickly generate\n",
    "summary stats.\n",
    "\n",
    "\n",
    "Let's next count the number of galaxies for each type. We can do this in a few\n",
    "ways, but we'll use `groupby` combined with a `count()` method.\n",
    "\n",
    "\n",
    "```python\n",
    "# count the number of samples by type\n",
    "type_counts = gal_df.groupby('type')['id'].count()\n",
    "```\n",
    "\n",
    "Or, we can also count just the rows that have type='U':\n",
    "\n",
    "```python\n",
    "gal_df.groupby('type')['id'].count()['U']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gal_df.groupby('type')['id'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gal_df.groupby('type')['id'].count()['U']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your turn\n",
    "\n",
    "1. What happens when you group by two columns using the following syntax and\n",
    "    then grab mean values:\n",
    "\t- `sorted_data = gal_df.groupby(['type','class'])`\n",
    "\t- `sorted_data.mean()`\n",
    "2. Get summary statistics on the number of votes for each galaxy class in your data. HINT: you can use the\n",
    "   following syntax to only create summary statistics for one column in your data\n",
    "   `name_of_dataframe['name_of_column'].describe()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another quick way to get the count of each unique value in a column is the `value_counts()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of entries for each class of galaxy\n",
    "gal_df['class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick & Easy Plotting Data Using Pandas\n",
    "\n",
    "We can plot our summary stats using Pandas, too. Check the [documentation](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.plot.html) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure figures appear inline in Jupyter Notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# create a scatter plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a histogram with .hist plotting option\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram from a groupby operation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_per_class = gal_df.groupby('class')['id'].nunique()\n",
    "\n",
    "# bar plots\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your turn\n",
    "\n",
    "You can choose what kind of plot to create (scatter, boxplot, histogram...).\n",
    "Remeber to check the [documentation](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.plot.html) for more details.\n",
    "\n",
    "1. Create a plot of average number votes across all classes of galaxies.\n",
    "2. Create a plot showing the distribution of number of votes for each type of galaxy\n",
    "3. Create a plot of number of votes vs p_el for each type of galaxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. bar plot of average number of votes for each class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. box plot of number of votes for each type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 3. Scatter plot of number of votes vs p_e grouped by type\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing & Slicing in Pandas\n",
    "\n",
    "[go back to the top](#Table-of-Contents)\n",
    "\n",
    "We often want to work with subsets of a **DataFrame** object. There are\n",
    "different ways to accomplish this including: using labels (ie, column headings - as used previously),\n",
    "numeric ranges or specific x,y index locations.\n",
    "\n",
    "\n",
    "**REMINDER**: Python Uses 0-based Indexing\n",
    "\n",
    "\n",
    "![indexing diagram](https://datacarpentry.org/python-ecology-lesson/fig/slicing-indexing.png)\n",
    "\n",
    "![slicing diagram](https://datacarpentry.org/python-ecology-lesson/fig/slicing-slicing.png)\n",
    "\n",
    "\n",
    "## Slicing using the `[]` operator\n",
    "\n",
    "Slicing using the `[]` operator selects a set of rows and/or columns from a\n",
    "DataFrame. To slice out a set of rows, you use the following syntax:\n",
    "`data[start:stop]`. When slicing in pandas the start bound is included in the\n",
    "output. The stop bound is one step BEYOND the row you want to select. So if you\n",
    "want to select rows 0, 1 and 2 your code would look like this:\n",
    "\n",
    "```python\n",
    "# select rows 0,1,2 (but not 3)\n",
    "gal_df[0:3]\n",
    "\n",
    "# select the first 5 rows (rows 0,1,\n",
    "2,3,4)\n",
    "gal_df[:5]\n",
    "\n",
    "# select the last element in the list\n",
    "gal_df[-1:]\n",
    "```\n",
    "\n",
    "To access columns we will need to call them by their labels, if you want more than one column you need to supply a list of column names.\n",
    "\n",
    "```python\n",
    "# selecting the if column\n",
    "gal_df['id']\n",
    "\n",
    "# selecting id and type\n",
    "gal_df[['id','type']]\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gal_df[0:5:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gal_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a single column we can select a subset of rows by either giving a list of rows or a slice\n",
    "gal_df['nvote'][[0,1,2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gal_df['nvote'][0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a subset of columns we need to specify a slice of rows\n",
    "gal_df[['nvote','type']][0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also reassign values within subsets of our DataFrame. But before we do that, let's make a \n",
    "copy of our DataFrame so as not to modify our original imported data. \n",
    "\n",
    "```python\n",
    "# copy the surveys dataframe so we don't modify the original DataFrame\n",
    "gal_copy = gal_df\n",
    "\n",
    "# set the first three rows of data in the DataFrame to 0\n",
    "gal_copy[0:3] = 0\n",
    "```\n",
    "\n",
    "Next, try the following code: \n",
    "\n",
    "```python\n",
    "gal_copy.head()\n",
    "gal_df.head()\n",
    "```\n",
    "What is the difference between the two data frames?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gal_copy = gal_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gal_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gal_copy[0:3]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gal_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gal_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Referencing Objects vs Copying Objects in Python\n",
    "\n",
    "We might have thought that we were creating a fresh copy of the `gal_df` objects when we \n",
    "used the code `surveys_copy = gal_df`. However the statement  y = x doesn’t create a copy of our DataFrame. \n",
    "It creates a new variable y that refers to the **same** object x refers to. This means that there is only one object \n",
    "(the DataFrame), and both x and y refer to it. So when we assign the first 3 columns the value of 0 using the \n",
    "`surveys_copy` DataFrame, the `gal_df` DataFrame is modified too. To create a fresh copy of the `gal_df`\n",
    "DataFrame we use the syntax y=x.copy(). But before we have to read the gal_df again because the current version contains the unintentional changes made to the first 3 columns.\n",
    "\n",
    "```python\n",
    "gal_df = pd.read_csv(\"data/GalaxyZoo1.csv\")\n",
    "gal_copy= gal_df.copy()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data back in and check it's correct\n",
    "gal_df = pd.read_csv(\"data/GalaxyZoo1.csv\")\n",
    "gal_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy data frame and check the copy\n",
    "gal_copy = gal_df.copy()\n",
    "gal_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modify copy and check both copy and original to see changes\n",
    "gal_copy[0:3]=0\n",
    "gal_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gal_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slicing and subsetting: label vs integer-based indexing\n",
    "\n",
    "We can select specific loctions or ranges of our data in both the row and column directions\n",
    "using either label or integer-based indexing.\n",
    "\n",
    "- `loc`: indexing via *labels* (which can be numbers)\n",
    "- `iloc`: indexing via *integers*\n",
    "\n",
    "\n",
    "![loc_iloc_subsetting](http://104.236.88.249/wp-content/uploads/2016/10/Pandas-selections-and-indexing.png)\n",
    "\n",
    "\n",
    "![dataframe_indexing](https://vrzkj25a871bpq7t1ugcgmn9-wpengine.netdna-ssl.com/wp-content/uploads/2019/01/pandas-dataframe-has-indexes.png)\n",
    "\n",
    "To select an index subset of rows AND columns from our DataFrame, we can use the `iloc`\n",
    "method. For example, we can select RA, Dec and number of votes (columns 2, 3 and 4 if we\n",
    "start counting at 1), by slicing our index like this:\n",
    "\n",
    "```python\n",
    "gal_df.iloc[0:3, 1:4]\n",
    "```\n",
    "\n",
    "**Note**: the order of selection is ROW followed by COLUMN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gal_df.iloc[0:3, 1:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we asked for a slice from 0:3. This yielded 3 rows of data. When you\n",
    "ask for 0:3, you are actually telling python to start at index 0 and select rows\n",
    "0, 1, 2 **up to but not including 3**.\n",
    "\n",
    "---\n",
    "\n",
    "Next let's explore subsetting our data using labels.\n",
    "**Note** When \"slicing\" labels, the start bound and the stop bound are **included**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select all columns for rows of index values 0 and 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Caution**  When using `loc`, integers *can* also be labels, but they refer to the **index label** and not the position. Thus when you use `loc`, and select rows 0:3, you will get a different result than using\n",
    "`iloc` to select rows 0:3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using .loc select rows 0:3 and columns ['ra', 'dec', 'nvote']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What happens when try to access a row label that does not exist using .loc?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try using iloc instead of loc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: If using `iloc` labels must be found in the DataFrame or you will get a `KeyError`. Using `loc` (at least for now) you'll get NaN entries returned so be careful!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsetting using masks\n",
    "\n",
    "A mask can be useful to locate where a particular subset of values exist or\n",
    "don't exist - for example,  NaN, or \"Not a Number\" values. To understand masks,\n",
    "we also need to understand `BOOLEAN` objects in python.\n",
    "\n",
    "Boolean values include `true` or `false`. So for example\n",
    "\n",
    "```python\n",
    "# set x to 5\n",
    "x = 5\n",
    "# what does the code below return?\n",
    "x > 5\n",
    "# how about this?\n",
    "x == 5\n",
    "```\n",
    "When we ask python what the value of `x > 5` is, we get `False`. This is because x\n",
    "is not greater than 5 it is equal to 5. To create a boolean mask, you first create the\n",
    "True / False criteria (e.g. values > 5 = True). Python will then assess each\n",
    "value in the object to determine whether the value meets the criteria (True) or\n",
    "not (False). Python creates an output object that is the same shape as\n",
    "the original object, but with a True or False value for each index location.\n",
    "\n",
    "You can use the syntax below when querying data from a DataFrame. Experiment\n",
    "with selecting various subsets of our data.\n",
    "\n",
    "* Equals: `==`\n",
    "* Not equals: `!=`\n",
    "* Greater than, less than: `>` or `<`\n",
    "* Greater than or equal to `>=`\n",
    "* Less than or equal to `<=`\n",
    "\n",
    "Let's try this out. \n",
    "We can select all rows that have 80 votes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#select all rows that have 80 votes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Or we can select all rows that do not contain 80 votes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We can define sets of criteria too:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's identify all locations in the survey data that have\n",
    "null (missing or NaN) data values. We can use the `isnull` method to do this.\n",
    "Each cell with a null value will be assigned a value of  `True` in the new\n",
    "boolean object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.isnull(gal_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To select the rows where there are null values,  we can use \n",
    "the mask as an index to subset our data as follows:\n",
    "\n",
    "```python\n",
    "#To select just the rows with NaN values, we can use the .any method\n",
    "gal_df[pd.isnull(gal_df).any(axis=1)]\n",
    "```\n",
    "\n",
    "We can run isnull on a particular column too. What does the code below do?\n",
    "\n",
    "```python\n",
    "# what does this do?\n",
    "empty_pE = gal_df[pd.isnull(gal_df['p_e'])]\n",
    "```\n",
    "\n",
    "Let's take a minute to look at the statement above. \n",
    "\n",
    "We are using the Boolean object as an index. \n",
    "We are asking python to select rows that have a `NaN` value for the probability of a galaxy being Elliptical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#To select just the rows with NaN values, we can use the .any method\n",
    "gal_df[pd.isnull(gal_df).any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what does this do?\n",
    "empty_pE = gal_df[pd.isnull(gal_df['p_e'])]\n",
    "empty_pE.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your turn\n",
    "\n",
    "\n",
    "1. You can use the `isin` command in python to query a DataFrame based upon a list of values as follows:\n",
    "   `gal_df[gal_df['class'].isin([listGoesHere])]`. Use the `isin` function to find all Elliptical galaxies of class E0, E1 and E7. How many records contain these values?\n",
    "2. The `~` symbol in Python can be used to return the OPPOSITE of the selection that you specify in python. It is equivalent to **is not in**. Write a query that selects all Spiral galaxies that are not classed as 'OTHER' an 'EDGE'.\n",
    "3. Create a new DataFrame that only contains valid (i.e. no NAN probability) observations of Spiral galaxies with over 100 votes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 -> count of E0,E1,E7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 -> all spirals that are not OTHER or EDGE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 -> spiral probaility is column 'p_s'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning our data for machine learning\n",
    "\n",
    "[[ go back to the top ]](#Table-of-Contents)\n",
    "\n",
    "## Problem definition\n",
    "\n",
    "We have been tasked to develop a machine learning model to classify galaxies by their morphology (appearance) from a dataset containing measurements such as:\n",
    "\n",
    "- redshift\n",
    "- magnitude in _ugriz_ bands\n",
    "- exponential scale radius and ellipticity\n",
    "- de Vaucouleurs scale radius and ellipticity\n",
    "- stellar mass\n",
    "- ...\n",
    "- etc.\n",
    "\n",
    "Galaxies in the dataset have labels taken from [Galaxy Zoo](https://www.galaxyzoo.org/) DR1 - Table 2. Galaxy Zoo is described in Lintott et al. 2008, MNRAS, 389, 1179 and the data release is described in Lintott et al. 2011, 410, 166. \n",
    "\n",
    "We use the final debiased labels to categorise a galaxy as:\n",
    "\n",
    "- spiral\n",
    "- elliptical\n",
    "\n",
    "![Galaxies](http://4.bp.blogspot.com/_rfhv4lPQhSY/TUQjfMLtrxI/AAAAAAAABAo/W3OF0mXZNZc/s1600/spiral%2Bor%2Belliptical%2Bzoo%2B1.jpg)\n",
    "\n",
    "<div style=\"text-align:center;font-weight:bold\">Figure: A spiral galaxy (left) and elliptical galaxy (right)</div>\n",
    "\n",
    "Our goal is to train a model that can accurately classify galaxies. We want our model to generalise well. i.e. it can correctly classify unseen galaxies (i.e. galaxies not in our training dataset).\n",
    "\n",
    "### Dataset\n",
    "\n",
    "For our introduction to machine learning, we will be using a subset of the data we worked with so far. To be able to try several machine learning algorithms we will be using a data set that combines [Galaxy Zoo DR1](https://www.galaxyzoo.org/) and the Sloan Digital Sky Survey (SDSS) ([using the DR9 SQL search](http://skyserver.sdss.org/dr9/en/tools/search/sql.asp)). \n",
    "\n",
    "The data dictionary for this dataset is presented in Table 1. This dataset is limited to the first 5,000 Galaxy Zoo classified galaxies which have spectra in the SDSS database. The debiased fraction of the votes in elliptical and spiral categories is given, along with columns identifying systems classified as spiral, elliptical or uncertain.\n",
    "\n",
    "<p style=\"text-align:center;font-weight:bold\">Table 1: Data dictionary</p>\n",
    "\n",
    "| Column           | Description                                                            |\n",
    "|:-----------------|:-----------------------------------------------------------------------|\n",
    "| id               | unique SDSS ID composed of [skyVersion, rerun, run, camcol, field, obj]|\n",
    "| ra               | right ascension  (HMS)                                                 |\n",
    "| dec              | declination (DMS)                                                      |\n",
    "| redshift         | redshift                                                               |\n",
    "| mag_u            | magnitude _u_ band                                                     |\n",
    "| mag_g            | magnitude _g_ band                                                     |\n",
    "| mag_r            | magnitude _r_ band                                                     |\n",
    "| mag_i            | magnitude _i_ band                                                     |\n",
    "| mag_z            | magnitude _z_ band                                                     |\n",
    "| deVRad_r         | de Vaucouleurs scale radius fit in _r_ band                            |\n",
    "| deVAB_r          | ellipticity from de Vaucouleurs fit in _r_ band                        |\n",
    "| expRad_r         | exponential scale radius fit in _r_ band                               |\n",
    "| expAB_r          | ellipticity from exponential fit in _r_ band                           |\n",
    "| stellar_mass     | log galaxy mass (in units of solar mass)                               |\n",
    "| votes            | number of Galaxy Zoo annotators                                        |\n",
    "| p_el_debiased    | debiased labelling probability the galaxy is elliptical                |\n",
    "| p_cs_debiased    | debiased labelling probability the galaxy is spiral                    |\n",
    "| spiral           | label = spiral galaxy {0=False, 1=True}                                |\n",
    "| elliptical       | label = elliptical galaxy {0=False, 1=True}                            |\n",
    "| uncertain        | label = uncertain {0=False, 1=True}                                    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understand and clean the data**\n",
    "\n",
    "The next step is to explore and familiarise ourselves with the data using the methods we learnedabove. \n",
    "Datasets can have errors, and it's important to identify and resolve these errors before rushing in to develop our ML models. Errors in the dataset that aren't resolved will propogate through the machine learning pipeline and will bias our results.\n",
    "\n",
    "Generally, we're looking to answer the following questions:\n",
    "\n",
    "* Is there anything wrong with the data?\n",
    "* Are there any quirks with the data?\n",
    "* Do I need to clean or remove some of the data?\n",
    "\n",
    "Let's start by reading the data into a `pandas` DataFrame and inspecting the first few rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas and check directory, \n",
    "# change directory if you need to\n",
    "import pandas as pd\n",
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data file, make sure you are trying to access the correct folder/file\n",
    "data = pd.read_csv('data/galaxies.csv')\n",
    "\n",
    "# Display the first few rows using the .head() method\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Missing Data Values - NaN**\n",
    "\n",
    "Dealing with missing data values is always a challenge, think about:\n",
    "* Why values are missing - was it because of a data entry error? Or are they data that someone was unable to collect? \n",
    "* How are the missing values represented? Are they empty cells in a spreadsheet, NaN or NA, or otherwise defined?\n",
    "* How should you treat the NaN? Should the value be 0 or the mean of all measurements, should they be ignored? \n",
    "\n",
    "Note: empty cells in a spreadhseet are automatically replaced with NaN by pandas when the data is read into a DataFrame.\n",
    "\n",
    " So, one of the first things we should do is to look for is missing data. We can use `pandas` to  display summary statistics for each (numerical) column as well as inspect rows with missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The .describe() method returns summary statistics for each (numerical) column\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get records containing any missing values\n",
    "data[data.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see a number of rows that contain missing values (`NaN`). There are many [methods for handling and imputing missing data](http://machinelearningmastery.com/handle-missing-data-python/). However, from our results we see that the galaxies with missing values have been labelled as `uncertain` so we will simply remove these records. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the rows containing any missing values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's double check to make sure the records have been removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should return 0 rows\n",
    "data[data.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the data with -9999 values, from our domain knowledge we know that this is a flag to signify NaN. So we can treat these as missing values / errors and remove them from our dataset as well.\n",
    "Also, having galaxies with masses or magnitudes less than 0 does not make sense and we should remove these records, too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look at and remove our data with values <0. \n",
    "\n",
    "Be careful, some data columns can be smaller than zero and still be valid measurements.\n",
    "\n",
    "Make sure to check any metadata given with your data to figure out what is a bad measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get records with ≤ 0 values using iloc\n",
    "# remember first number in slice is inclusive, second number is exclusive\n",
    "# Exclude the first  4 columns (id, ra, dec, redshift) and last 5 columns (galaxy labels and probabilities) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only retain records with > 0 values in our dataset\n",
    "\n",
    "# Let's check the summary stats again\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to construct a good training dataset for developing a robust model. In machine learning, we also refer to the training dataset as the ground truth. Any uncertainities or errors in the ground truth can confuse and reduce the performance of our models.\n",
    "\n",
    "In order to build a good training dataset, we only want to keep records where we are confident there is agreement between the Galaxy Zoo annotators in labelling galaxies. From the dataset, it appears we can achieved this by:\n",
    "\n",
    "- Keeping records where the uncertain label column is equal to False (0)\n",
    "\n",
    "Let's try this method and perform some sanity checking to see if the resulting data subset has debiased probabilities for ellipital and spiral labels ≥ 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Filter records using the uncertain column \n",
    "\n",
    "\n",
    "\n",
    "# 2. Filter records using the debiased probabilities ≥ 0.5\n",
    "\n",
    "\n",
    "\n",
    "# Print the lengths for comparison\n",
    "print('# records after removing uncertain rows: %d' % (len(f1)))\n",
    "print('# records with ≥ 0.5 debiased probability: %d' % (len(f2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There appears to be a discrepency of 5 records between the two data subsets. Let's investigate further by inspecting these records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display filter #1 rows that don't appear in the filter #2 dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These records have been labelled as spiral galaxies but the debiased probabilities (confidence) for these labels are below the 0.5 confidence threshold. i.e. `p_cs_debiased < 0.5`. Additionally, for the last 2 records, we see that debiased probabilities for labelling the galaxy as elliptical is actually larger than the spiral probabilities which suggests these might be due to labelling errors. \n",
    "\n",
    "It appears we can't rely on filtering the dataset on the `uncertain` column alone so we will apply both filters to clean the dataset. Sanity checking your dataset is a step that is regularly overlooked but dedicating time to explore and resolve these errors can (sometimes dramatically) improve the performance of our machine learning models.\n",
    "\n",
    "We will make a copy of the filtered dataset in the variable `df` (abbreviation for DataFrame) for our clean dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use .copy to create and independent copy of the dataframe\n",
    "df = f2.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's clean and prepare our dataset by:\n",
    "\n",
    "- creating a class column to represent the galaxy type label `{0=elliptical, 1=spiral}`\n",
    "- removing unwanted columns that won't be used in our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the class column - do this by renaming the 'spiral' column to 'class'\n",
    "\n",
    "# Remove unwanted columns\n",
    "unwanted_columns = \n",
    "\n",
    "# Inspect the first few rows of our prepared dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, it's always a good idea to look to have a closer look at our data — especially for outliers. Let's start by printing out some summary statistics about the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display dataset summary statistics\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The summary table provides some useful information:\n",
    "\n",
    "- there are 1,797 galaxies in our dataset\n",
    "- the mean of the `class` column indicates that ~75.3% of the dataset are spiral galaxies\n",
    "- min values for the radius columns range from 0.31 to 0.38\n",
    "- min value for the `stellar_mass` = 8.096\n",
    "- min value for the `mag` ranging from 15.59 to 18.5\n",
    "\n",
    "Our cleaned dataset contains 1,797 galaxies and 75.3% of the records are spiral galaxies.\n",
    "\n",
    "Tables like this are useful when we know that our data should fall in a particular range. e.g. It did not make sense to have zero or negative values for some of our columns. However, it is usually better to visualize the data in some way. Visualization makes outliers and errors immediately stand out, whereas they might go unnoticed in a large table of numbers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Command to show plots inside of the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a **scatterplot matrix**. Scatterplot matrices plot the distribution of each column along the diagonal and a scatterplot matrix for the combination of each variable. They make for an efficient tool to look for errors in our data.\n",
    "\n",
    "Since we have quite a few columns, let's generate two scatterplots for data containing:\n",
    "\n",
    "1.  magnitudes\n",
    "2. measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magnitudes scatterplot\n",
    "magnitudes = ['mag_u', 'mag_g', 'mag_r', 'mag_i', 'mag_z']\n",
    "selected_columns = magnitudes + ['class']\n",
    "sb.pairplot(df[selected_columns], hue='class', plot_kws={'alpha': 0.3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the magnitude scatterplot matrix, we see a potential outlier: the `mag_u` value of an `elliptical` galaxy falls outside its normal range (a value of ~24) and there seems to be an outlier for the spirals as well with values >18 for all(?) magnitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect possible outliers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is nothing obvious that leads us to believe the records are erroneous. Let's keep them in our dataset.\n",
    "\n",
    "Next we have a look at the distribution of the other measurements in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measurements scatterplot\n",
    "measurements = ['redshift', 'stellar_mass', 'deVRad_r', 'deVAB_r', 'expRad_r', 'expAB_r']\n",
    "selected_columns = measurements + ['class']\n",
    "sb.pairplot(df[selected_columns], hue='class', plot_kws={'alpha': 0.3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the scatterplot matrix, we see potential outliers: \n",
    "\n",
    "- the `stellar_mass` value of a `spiral` galaxy falls outside its normal range (a value of ~8).\n",
    "- two spiral galaxies with `expRad_r` ≥ 17\n",
    "\n",
    "Let's inspect the outlier records to determine whether if we should keep them in our training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect possible outliers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is nothing obvious that leads us to believe the records are erroneous. Let's keep them in our dataset.\n",
    "\n",
    "Fixing outliers can be difficult. It's often unclear whether the outlier was caused by measurement error, recording the data in improper units, or if the outlier is a real anomaly. For that reason, we should be careful when working with outliers. If we decide to exclude any data, we should document what data was excluded and provide reasons for excluding that data. i.e. The data didn't fit my hypothesis will not stand peer review.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Out Data to CSV\n",
    "\n",
    "After all this hard work, we don't want to repeat the data cleaning process every time we work with this dataset. Let's save the cleaned dataset as a separate file and work directly with that data file from now on.\n",
    "\n",
    "We can use the `to_csv` command to do export a DataFrame in CSV format. \n",
    "Note that by default the data will be saved into the current working directory. \n",
    "We can save it to a different folder by adding the foldername and a slash before the file name,\n",
    "either using relative or absolute path names.\n",
    "\n",
    "```python\n",
    "# Write DataFrame to CSV \n",
    "df.to_csv('galaxies-clean.csv')\n",
    "```\n",
    "\n",
    "Check out your working directory to make sure the CSV wrote out properly, and\n",
    "that you can open it! If you want, try to bring it back into python to make sure\n",
    "it imports properly.\n",
    "\n",
    "```python\t\n",
    "# for kicks read our output back into python and make sure all looks good\n",
    "new_output = pd.read_csv('galaxies-clean.csv', keep_default_na=False, na_values=[\"NA\"])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the clean dataset to a file (try saving it into the correct folder for tomorrows/DAY3 lesson)\n",
    "df.to_csv('galaxies-clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory analysis\n",
    "[[ go back to the top ]](#Table-of-Contents)\n",
    "\n",
    "Exploratory analysis is the step where we start delving deeper into the data set beyond the outliers and errors. We'll be looking to answer questions such as:\n",
    "\n",
    "- How is my data distributed?\n",
    "- Are there any correlations in my data?\n",
    "- Are there any confounding factors that explain these correlations?\n",
    "\n",
    "This is the stage where we plot the data in as many ways as possible. Create many charts, but don't bother making them pretty as these charts are for internal use.\n",
    "\n",
    "For completeness, let's generate a scatterplot for all features in one figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Command to show plots inside of the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "\n",
    "magnitudes = ['mag_u', 'mag_g', 'mag_r', 'mag_i', 'mag_z']\n",
    "measurements = ['redshift', 'stellar_mass', 'deVRad_r', 'deVAB_r', 'expRad_r', 'expAB_r']\n",
    "\n",
    "\n",
    "# Plot scatterplot matrix with all features\n",
    "selected_columns = measurements + magnitudes + ['class']\n",
    "sb.pairplot(df[selected_columns], hue='class', plot_kws={'alpha': 0.3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make a number of observations from this plot:\n",
    "\n",
    "1. `reshift`, `stellar_mass`, `mag_u` and `mag_g` follow a normal distribution. This is something to keep in mind if we use modelling methods that assume the data is normally distributed.\n",
    "\n",
    "2. There is a strong positive correlation between the `expAB_r` and `deVAB_r` which indicates we might not need to use both of these features in our model.\n",
    "\n",
    "3. There is a strong positive correlation between the `mag_r`, `mag_i` and `mag_z`.\n",
    "\n",
    "4. There is a positive correlation between `mag_u` and `mag_g` with the other magnitude bands but not as strong as in observation #3. This suggests these features might provide added information not present in the `mag_r`, `mag_i` and `mag_z` bands.\n",
    "\n",
    "5. There are no clear pair-wise combinations of features that can easily seperate the two classes. \n",
    "\n",
    "Distinguishing between `spiral` and `elliptical` galaxies might be difficult given how much these features interrelate.\n",
    "\n",
    "We can also make **violin plots** of the data to compare the measurement distributions of the classes. Violin plots contain the same information as [box plots](https://en.wikipedia.org/wiki/Box_plot), but also scales the box according to the density of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General violin plots of all features\n",
    "selected_columns = measurements + magnitudes + ['class']\n",
    "plt.figure(figsize=(13, 13))\n",
    "for column_index, column in enumerate(selected_columns):\n",
    "    if column == 'class':\n",
    "        continue\n",
    "    plt.subplot(3, 4, column_index + 1)\n",
    "    sb.violinplot(x='class', y=column, data=df[selected_columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are numerous errors and possible scenarios you may face while exploring and cleaning your data.\n",
    "\n",
    "The general takeaways here should be:\n",
    "\n",
    "- Handle missing data: replace it if you can or drop it\n",
    "- Ensure your data is encoded properly\n",
    "- Check if your data falls within the expected range and use domain knowledge whenever possible to define that expected range\n",
    "- Avoid tidying your data manually because that is not easily reproducible\n",
    "- Use code as a record of how you tidied your data\n",
    "- Plot everything you can about the data at this stage of the analysis so you can visually confirm everything looks correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supplement: Merging DataFrames\n",
    "\n",
    "[[ go back to the top ]](#Table-of-Contents)\n",
    "\n",
    "In many \"real world\" situations, the data that we want to use come in multiple\n",
    "files. We often need to combine these files into a single DataFrame to analyze\n",
    "the data. The pandas package provides [various methods for combining\n",
    "DataFrames](http://pandas.pydata.org/pandas-docs/stable/merging.html) including\n",
    "`merge` and `concat`.\n",
    "\n",
    "To work through the examples below, we first need to load the galaxy zoo and\n",
    "SDSS files into pandas DataFrames. In iPython:\n",
    "\n",
    "``` python\n",
    "import pandas as pd\n",
    "gal_df = pd.read_csv('GalaxyZooSub.csv',\n",
    "                         keep_default_na=False, na_values=[\"NA\"])\n",
    "sdss = pd.read_csv('SDSS_blue_centre_query.csv',\n",
    "                         keep_default_na=False, na_values=[\"\"])\n",
    "gal_df.dtypes\n",
    "sdss.dtypes\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "gal_df = pd.read_csv('data/GalaxyZooSub.csv',\n",
    "                         keep_default_na=False, na_values=[\"NA\"])\n",
    "sdss = pd.read_csv('data/SDSS_blue_centre_query.csv',\n",
    "                         keep_default_na=False, na_values=[\"\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gal_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdss.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take note that the `read_csv` method we used can take some additional options which\n",
    "we didn't use previously. Many functions in python have a set of options that\n",
    "can be set by the user if needed. In this case, we have told Pandas to assign\n",
    "empty values in our CSV to NaN `keep_default_na=False, na_values=[\"\"]`.\n",
    "[http://pandas.pydata.org/pandas-docs/dev/generated/pandas.io.parsers.read_csv.html](More\n",
    "about all of the read_csv options here.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatinating\n",
    "\n",
    "We can use the `concat` function in Pandas to append either columns or rows from\n",
    "one DataFrame to another.  Let's grab two subsets of our data to see how this\n",
    "works.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in first 10 lines of gal_df table\n",
    "gal_sub=gal_df.head(10)\n",
    "gal_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab the last 10 rows (minus the last one)\n",
    "gal_last10=gal_df[-11:-1]\n",
    "gal_last10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset the index values to the second dataframe appends properly\n",
    "# drop=True option avoids adding new index column with old index values\n",
    "gal_last10 = gal_last10.reset_index(drop=True)\n",
    "gal_last10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we concatenate DataFrames, we need to specify the axis. `axis=0` tells\n",
    "Pandas to stack the second DataFrame under the first one. It will automatically\n",
    "detect whether the column names are the same and will stack accordingly.\n",
    "`axis=1` will stack the columns in the second DataFrame to the RIGHT of the\n",
    "first DataFrame. To stack the data vertically, we need to make sure we have the\n",
    "same columns and associated column format in both datasets. When we stack\n",
    "horizonally, we want to make sure what we are doing makes sense (ie the data are\n",
    "related in some way).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack the DataFrames on top of each other\n",
    "vertical_stack = pd.concat([gal_sub,gal_last10], axis=0)\n",
    "vertical_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# place the DataFrames side by side\n",
    "horizontal_stack = pd.concat([gal_sub,gal_last10], axis=1)\n",
    "horizontal_stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your turn \n",
    "**Row Index Values and Concat**\n",
    "\n",
    "Have a look at the `vertical_stack` dataframe? Notice anything unusual?\n",
    "The row indexes for the two data frames `survey_sub` and `survey_sub_last10`\n",
    "have been repeated. We can reindex the new dataframe using the `reset_index()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joining DataFrames\n",
    "\n",
    "When we concatenated our DataFrames we simply added them to each other -\n",
    "stacking them either vertically or side by side. Another way to combine\n",
    "DataFrames is to use columns in each dataset that contain common values (a\n",
    "common unique id). Combining DataFrames using a common field is called\n",
    "\"joining\". The columns containing the common values are called \"join key(s)\".\n",
    "Joining DataFrames in this way is often useful when one DataFrame is a \"lookup\n",
    "table\" containing additional data that we want to include in the other. \n",
    "\n",
    "For example, the `SDSS_blue_centre_query.csv` file that we've been loaded is a query from the SDSS DR7 database for 1000 galaxies with blue centres (things we would expect to be Spirals). \n",
    "This table contains the ID, postition, redshift and r-band magnitude as well as size measurements.\n",
    "\n",
    "\n",
    "## Identifying join keys\n",
    "\n",
    "To identify appropriate join keys we first need to know which field(s) are\n",
    "shared between the files (DataFrames). We might inspect both DataFrames to\n",
    "identify these columns. If we are lucky, both DataFrames will have columns with\n",
    "the same name that also contain the same data. If we are less lucky, we need to\n",
    "identify a (differently-named) column in each DataFrame that contains the same\n",
    "information.\n",
    "\n",
    "Check the column names for gal_df and sdss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gal_df.columns.values)\n",
    "print(sdss.columns.values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example, the join key is the column containing the object\n",
    "identifier. We could also use 'ra' and 'dec', however, they are not presented in the same units thus joining on id will be esier.\n",
    "\n",
    "There are [different types of joins](http://blog.codinghorror.com/a-visual-explanation-of-sql-joins/), so we\n",
    "also need to decide which type of join makes sense for our analysis.\n",
    "\n",
    "## Inner joins\n",
    "\n",
    "The most common type of join is called an _inner join_. An inner join combines\n",
    "two DataFrames based on a join key and returns a new DataFrame that contains\n",
    "**only** those rows that have matching values in *both* of the original\n",
    "DataFrames. \n",
    "\n",
    "Inner joins yield a DataFrame that contains only rows where the value being\n",
    "joins exists in BOTH tables. An example of an inner join, adapted from [this\n",
    "page](http://blog.codinghorror.com/a-visual-explanation-of-sql-joins/) is below:\n",
    "\n",
    "![Inner join -- courtesy of codinghorror.com](http://blog.codinghorror.com/content/images/uploads/2007/10/6a0120a85dcdae970b012877702708970c-pi.png)\n",
    "\n",
    "The pandas function for performing joins is called `merge` and an inner join is\n",
    "the default option:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_inner = pd.merge(left=gal_df,right=sdss, left_on='id', right_on='objID')\n",
    "# if both column names were 'id', we could skip the `left_on`\n",
    "# and `right_on` arguments and still get the same result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what's the size of the output data?\n",
    "merged_inner.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_inner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of an inner join of `gal_df` and `sdss` is a new DataFrame\n",
    "that contains the combined set of columns from `gal_df` and `sdss`. It\n",
    "*only* contains rows that have an ID that is the same in\n",
    "both the `gal_df` and `sdss` DataFrames. \n",
    "\n",
    "In other words, if a row in\n",
    "`gal_df` has a value of `id` that does *not* appear in the `objID`\n",
    "column of `sdss`, it will not be included in the DataFrame returned by an\n",
    "inner join.  Similarly, if a row in `sdss` has a value of `objID`\n",
    "that does *not* appear in the `id` column of `gal_df`, that row will not\n",
    "be included in the DataFrame returned by an inner join.\n",
    "\n",
    "The two DataFrames that we want to join are passed to the `merge` function using\n",
    "the `left` and `right` argument. The `left_on='id'` argument tells `merge`\n",
    "to use the `id` column as the join key from `gal_df` (the `left`\n",
    "DataFrame). Similarly , the `right_on='objID'` argument tells `merge` to\n",
    "use the `objID` column as the join key from `sdss` (the `right`\n",
    "DataFrame). For inner joins, the order of the `left` and `right` arguments does\n",
    "not matter.\n",
    "\n",
    "The result `merged_inner` DataFrame contains all of the columns from `survey_sub` as well as all the columns from `sdss`.\n",
    "\n",
    "Notice that `merged_inner` has way fewer rows than `gal_df`. This is an\n",
    "indication that there were rows in `gal_df` with value(s) for `id` that\n",
    "do not exist as value(s) for `objID` in `sdss`.\n",
    " \n",
    "## Left joins\n",
    "\n",
    "What if we want to add information from `sdss` to `gal_df` without\n",
    "losing any of the information from `gal_df`? In this case, we use a different\n",
    "type of join called a \"left outer join\", or a \"left join\".\n",
    "\n",
    "Like an inner join, a left join uses join keys to combine two DataFrames. Unlike\n",
    "an inner join, a left join will return *all* of the rows from the `left`\n",
    "DataFrame, even those rows whose join key(s) do not have values in the `right`\n",
    "DataFrame.  Rows in the `left` DataFrame that are missing values for the join\n",
    "key(s) in the `right` DataFrame will simply have null (i.e., NaN or None) values\n",
    "for those columns in the resulting joined DataFrame.\n",
    "\n",
    "Note: a left join will still discard rows from the `right` DataFrame that do not\n",
    "have values for the join key(s) in the `left` DataFrame.\n",
    "\n",
    "![Left Join](http://blog.codinghorror.com/content/images/uploads/2007/10/6a0120a85dcdae970b01287770273e970c-pi.png)\n",
    "\n",
    "A left join is performed in pandas by calling the same `merge` function used for\n",
    "inner join, but using the `how='left'` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_left = pd.merge(left=gal_df,right=sdss, left_on='id', right_on='objID', how='left')\n",
    "\n",
    "merged_left"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result DataFrame from a left join (`merged_left`) looks very much like the\n",
    "result DataFrame from an inner join (`merged_inner`) in terms of the columns it\n",
    "contains. However, unlike `merged_inner`, `merged_left` contains the **same\n",
    "number of rows** as the original `gal_df` DataFrame. When we inspect\n",
    "`merged_left`, we find there are rows where the information that should have\n",
    "come from `sdss` (e.g., `z`, `r`, and `expRad_r`) is\n",
    "missing (they contain NaN values):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(merged_left[ pd.isnull(merged_left.z) ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These rows are the ones where the value of `id` from `sdss` does not occur in `gal_df`.\n",
    "\n",
    "\n",
    "## Other join types\n",
    "\n",
    "The pandas `merge` function supports two other join types:\n",
    "\n",
    "* Right (outer) join: Invoked by passing `how='right'` as an argument. Similar\n",
    "  to a left join, except *all* rows from the `right` DataFrame are kept, while\n",
    "  rows from the `left` DataFrame without matching join key(s) values are\n",
    "  discarded.\n",
    "* Full (outer) join: Invoked by passing `how='outer'` as an argument. This join\n",
    "  type returns the all pairwise combinations of rows from both DataFrames; i.e.,\n",
    "  the result DataFrame will `NaN` where data is missing in one of the dataframes. This join type is\n",
    "  very rarely used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
