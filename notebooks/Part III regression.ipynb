{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANITA astroinformatics summer school 2019 - \"Rise of the machines\"\n",
    "\n",
    "## Part III - Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides an introduction to machine learning and walks through how to develop workflows for training machine learning models.\n",
    "\n",
    "This lesson is prepared by:\n",
    "- [Kevin Chai](http://computation.curtin.edu.au/about/computational-specialists/health-sciences/)\n",
    "- [Rebecca Lange](http://computation.curtin.edu.au/about/computational-specialists/humanities/)\n",
    "\n",
    "from the [Curtin Institute for Computation](http://computation.curtin.edu.au) at Curtin University in Perth, Australia. \n",
    "\n",
    "Some of the materials in this notebook have been referenced and adapted from:\n",
    "- [Randal Olsen's Data Science Notebook](https://github.com/rhiever/Data-Analysis-and-Machine-Learning-Projects/tree/master/example-data-science-notebook)\n",
    "- [Sebastian Raschka's Python Machine Learning Notebooks](https://github.com/rasbt/python-machine-learning-book)\n",
    "- [Kevin Markham's Scikit Learn Notebooks](https://github.com/justmarkham/scikit-learn-videos)\n",
    "\n",
    "Make sure to open this notebook in the root directory of the code repository.\n",
    "\n",
    "This work is made available under the [Creative Commons Attribution 4.0 International License](http://creativecommons.org/licenses/by/4.0/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. [Problem definition](#1.-Problem-definition)\n",
    "2. [Prepare the dataset](#2.-Prepare-the-dataset)\n",
    "3. [Train models](#3.-Train-models)\n",
    "4. [Feature normalisation](#4.-Feature-normalisation)\n",
    "5. [Feature engineering](#5.-Feature-engineering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[ go back to the top ]](#Table-of-contents)\n",
    "\n",
    "This notebook uses several Python packages that come standard with the [Anaconda Python distribution](http://continuum.io/downloads). The primary libraries that we'll be using are:\n",
    "\n",
    "* **NumPy**: a fast numerical array structure and helper functions.\n",
    "* **pandas**: a DataFrame structure to store data in memory and work with it easily and efficiently.\n",
    "* **scikit-learn**: a machine learning package.\n",
    "* **matplotlib**: a basic plotting library; most other plotting libraries are built on top of it.\n",
    "* **seaborn**: a advanced statistical plotting library.\n",
    "\n",
    "To make sure you have all of the packages you need, install them with `conda`:\n",
    "\n",
    "    conda install numpy pandas scikit-learn matplotlib seaborn\n",
    "\n",
    "`conda` may ask you to update some of the packages if you don't have the most recent version. Allow it to do so.\n",
    "\n",
    "Alternatively, if you can install the packages with [pip](https://pip.pypa.io/en/stable/installing/) (a Python package manager):\n",
    "\n",
    "    pip install numpy pandas scikit-learn matplotlib seaborn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Problem definition\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your colleague has heard about your recent success in applying machine learning for galaxy classification. They mention they have a dataset containing _ugriz_ photomerty of galaxies but no redshift measurements. They would like you to develop a machine learning model to estimate the redshift values.\n",
    "\n",
    "This is a regression problem. Regression is a supervised learning approach but instead of predicting the category of an example, we are predicting a continuous value. i.e. $y_i \\in\\ \\mathbb{R}$. For example, the predicted redshift of galaxy `i` is 0.2312.\n",
    "\n",
    "Developing a regression model follows the same process as building a classification model with the exception of using a different metric for evaluating the model performance. i.e. we can't use classification accuracy for evaluating our regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare the dataset\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and prepare the dataset for using the scikit-learn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Set a random seed number to reproduce our results\n",
    "seed = 11\n",
    "\n",
    "# 1. Load the clean dataset using pandas read_csv function\n",
    "df = pd.read_csv('data/galaxies-clean.csv')\n",
    "\n",
    "# 2. Select the columns of interest for modelling\n",
    "features = ['mag_u','mag_g','mag_r','mag_i','mag_z']\n",
    "\n",
    "# 3. Create the features matrix as X\n",
    "X = df[features]\n",
    "\n",
    "# 4. Create the labels vector as y\n",
    "y = df['redshift']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Split the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a training and test dataset using the `train_test_split` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/268964i/anaconda3/envs/deepml/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Import the function\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into X_train, X_test, y_train, y_test\n",
    "# Use a training dataset size of 80%\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8,\n",
    "                                                   random_state = seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train models\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will fit the regression models without any parameter tuning using the following three models:\n",
    "\n",
    "- [Linear Regression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)\n",
    "- [RandomForestRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)\n",
    "- [MLPRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html)\n",
    "\n",
    "We will measure model performance using the root mean squared error (RMSE) metric. This can be calculated by using the NumPy function `np.sqrt` and the scikit-learn `mean_squared_error` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/268964i/anaconda3/envs/deepml/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression: 0.022\n",
      "Random forest regressor: 0.026\n",
      "Multi layer perceptron: 0.047\n",
      "Baysian Ridge: 0.022\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import the classes\n",
    "from sklearn.linear_model import LinearRegression, BayesianRidge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Step 2: Instantiate the estimators\n",
    "lr = LinearRegression()\n",
    "rf = RandomForestRegressor(random_state=seed)\n",
    "mlp = MLPRegressor(random_state=seed)\n",
    "br = BayesianRidge()\n",
    "\n",
    "# Step 3: Fit the estimators on data (i.e. train the models)\n",
    "lr.fit(X_train, y_train)\n",
    "rf.fit(X_train, y_train)\n",
    "mlp.fit(X_train, y_train)\n",
    "br.fit(X_train, y_train)\n",
    "\n",
    "# Step 4: Generate predictions\n",
    "y_pred_m1 = lr.predict(X_test)\n",
    "y_pred_m2 = rf.predict(X_test)\n",
    "y_pred_m3 = mlp.predict(X_test)\n",
    "y_pred_m4 = br.predict(X_test)\n",
    "\n",
    "# Calculate the Root Mean Squared Error (RMSE)\n",
    "# np.sqrt(mean_squared_error(...))\n",
    "m1_score = np.sqrt(mean_squared_error(y_test,y_pred_m1))\n",
    "m2_score = np.sqrt(mean_squared_error(y_test,y_pred_m2))\n",
    "m3_score = np.sqrt(mean_squared_error(y_test,y_pred_m3))\n",
    "m4_score = np.sqrt(mean_squared_error(y_test,y_pred_m4))\n",
    "\n",
    "# Display the model scores\n",
    "print('Linear regression: %.3f' % (m1_score))\n",
    "print('Random forest regressor: %.3f' % (m2_score))\n",
    "print('Multi layer perceptron: %.3f' % (m3_score))\n",
    "print('Baysian Ridge: %.3f' % (m4_score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear regression model is off by an average of 0.022 redshift while the random forest regression has an RMSE of 0.026. The multi layer perceptron (without any parameter tuning) achieves a poorer RMSE of 0.047.\n",
    "\n",
    "Let's examine the correlation between the predicted and actual redshift values for both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression: 0.888\n",
      "Random forest regressor: 0.838\n",
      "Multi layer perceptron: 0.602\n",
      "Bayesian Ridge: 0.888\n"
     ]
    }
   ],
   "source": [
    "# calculate the correlation between the labels and predictions\n",
    "m1_corr = np.corrcoef(y_test, y_pred_m1)[0][1]\n",
    "m2_corr = np.corrcoef(y_test, y_pred_m2)[0][1]\n",
    "m3_corr = np.corrcoef(y_test, y_pred_m3)[0][1]\n",
    "m4_corr = np.corrcoef(y_test, y_pred_m4)[0][1]\n",
    "\n",
    "print('Linear regression: %.3f' % (m1_corr))\n",
    "print('Random forest regressor: %.3f' % (m2_corr))\n",
    "print('Multi layer perceptron: %.3f' % (m3_corr))\n",
    "print('Bayesian Ridge: %.3f' % (m4_corr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's impressive. The linear regression model has a correlation value of 0.888 while the multi layer perceptron has the lowest correlation of 0.6.\n",
    "\n",
    "Let's look at the generated coefficients and intercept values for the linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.2941654156646464\n",
      "[-0.02794643  0.25568438 -0.13844462 -0.37345313  0.3028921 ]\n"
     ]
    }
   ],
   "source": [
    "print(lr.intercept_)\n",
    "print(lr.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mag_u', -0.027946434186041844),\n",
       " ('mag_g', 0.2556843840450466),\n",
       " ('mag_r', -0.13844461806634395),\n",
       " ('mag_i', -0.37345313161319504),\n",
       " ('mag_z', 0.3028921026506884)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pair coefficients with feature names\n",
    "list(zip(features, lr.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that the linear regression model for estimating the redshift of a galaxy is:\n",
    "\n",
    "<p style=\"text-align:center;font-weight:bold\">$redshift = -0.027(mag_u) + 0.255(mag_g) - 0.138 (mag_r) - 0.373(mag_i) + 0.302(mag_z) - 0.294$</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature normalisation\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "\n",
    "An important data preparation step for many machine learning models is to [normalise / standarised the feature values](http://scikit-learn.org/stable/modules/preprocessing.html) in our dataset. e.g. we can scale the magnitude values to a smaller and standardised range of [0, 1]. Note: this is not required for decision trees and random forests as those models are scale invariant.\n",
    "\n",
    "We can normalise our features to a range of [0, 1] using the scikit-learn `MinMaxScaler()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original scale:\n",
      "  X_train 11.729 - 24.691\n",
      "  X_test 12.357 - 21.861\n",
      "Transformed scale:\n",
      "  X_train 0.000 - 1.000\n",
      "  X_test 0.014 - 0.895\n"
     ]
    }
   ],
   "source": [
    "# Import the MinMaxScaler function\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Fit the scaler on the training dataset ONLY\n",
    "scaler = MinMaxScaler().fit(X_train)\n",
    "\n",
    "# Transform both the training and test datasets\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Print out the new scaled values\n",
    "print('Original scale:')\n",
    "print('  X_train %.3f - %.3f' % (np.min(np.min(X_train)), np.max(np.max(X_train))))\n",
    "print('  X_test %.3f - %.3f' % (np.min(np.min(X_test)), np.max(np.max(X_test))))\n",
    "print('Transformed scale:')\n",
    "print('  X_train %.3f - %.3f' % (np.min(X_train_scaled), np.max(X_train_scaled)))\n",
    "print('  X_test %.3f - %.3f' % (np.min(X_test_scaled), np.max(X_test_scaled)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now fit a linear regression and multi layer perceptron model on the scaled dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regresssion scaled\n",
      "  RMSE: 0.022\n",
      "  correlation: 0.888\n",
      "Multi layer perception scaled\n",
      "  RMSE: 0.034\n",
      "  correlation: 0.690\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the estimators\n",
    "lr_scaled = LinearRegression()\n",
    "mlp_scaled = MLPRegressor(random_state=seed)\n",
    "\n",
    "# Fit the estimators\n",
    "lr_scaled.fit(X_train_scaled,y_train)\n",
    "mlp_scaled.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Step 4: Generate predictions\n",
    "y_pred_m1 = lr_scaled.predict(X_test_scaled)\n",
    "y_pred_m2 = mlp_scaled.predict(X_test_scaled)\n",
    "\n",
    "# Calculate the Root Mean Squared Error (RMSE)\n",
    "# np.sqrt(mean_squared_error(...))\n",
    "m1_score = np.sqrt(mean_squared_error(y_test, y_pred_m1))\n",
    "m2_score = np.sqrt(mean_squared_error(y_test, y_pred_m2))\n",
    "\n",
    "# calculate the correlation coefficient\n",
    "m1_corr = np.corrcoef(y_test,y_pred_m1)[0][1]\n",
    "m2_corr = np.corrcoef(y_test, y_pred_m2)[0][1]\n",
    "\n",
    "# print the results\n",
    "print('Linear regresssion scaled')\n",
    "print('  RMSE: %.3f' % (m1_score))\n",
    "print('  correlation: %.3f' % (m1_corr))\n",
    "print('Multi layer perception scaled')\n",
    "print('  RMSE: %.3f' % (m2_score))\n",
    "print('  correlation: %.3f' % (m2_corr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear regression model achieved the same performance on the scaled and unscaled dataset (0.022 RMSE). However, the multi layer perceptron model has an improved RMSE of 0.034 (previously 0.047) and correlation coefficient of 0.69 (previously 0.602) on the scaled dataset.\n",
    "\n",
    "Normalisation might not always improve performance but it is common practice to do so before training certain types of models. You should also think about your dataset and what features you are normalising. e.g. are the features measured on a linear scale? Should magnitude be normalised or should the raw fluxes be normalised instead?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature engineering\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "\n",
    "Another approach to improve the performance of our models is to try new features based on our knowledge of the domain. This is known as feature engineering.\n",
    "\n",
    "We are currently using the raw magnitude bands as features to estimate a galaxy's redshift. However, it might make more sense to use colour features that measure the ratio of flux in neighbouring filters. This is equivalent to subtracting the magnitudes of the neighbouring filters. The key to photometric red shift is that a red shifted galaxy will have different observed colors to what it would have at red shift zero. i.e. galaxies at higher redshift tend to be redder in colour. \n",
    "\n",
    "With this knowledge, let's build a model with 4 engineered features by subtracting neighbouring bands in the `ugriz` magnitude channels:\n",
    "\n",
    "- $mag_u - mag_g$\n",
    "- $mag_g - mag_r$\n",
    "- $mag_r - mag_i$\n",
    "- $mag_i - mag_z$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of the features matrix\n",
    "X_new = X.copy()\n",
    "\n",
    "# Create new features\n",
    "# subtract neighbouring magnitudes\n",
    "X_new['u-g'] = X_new['mag_u'] - X_new['mag_g']\n",
    "X_new['g-r'] = X_new['mag_g'] - X_new['mag_r']\n",
    "X_new['r-i'] = X_new['mag_r'] - X_new['mag_i']\n",
    "X_new['i-z'] = X_new['mag_i'] - X_new['mag_z']\n",
    "\n",
    "# Remove the old columns\n",
    "X_new.drop(['mag_u', 'mag_g', 'mag_r', 'mag_i', 'mag_z'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit a linear regression and a multi layer perceptron model with the new features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regresssion new features\n",
      "  RMSE: 0.025\n",
      "  correlation: 0.846\n",
      "Multi layer perceptron new features\n",
      "  RMSE: 0.026\n",
      "  correlation: 0.845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/268964i/anaconda3/envs/deepml/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into X_train, X_test, y_train, y_test\n",
    "X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(X_new, y, train_size=0.8, \n",
    "                                                                    random_state=seed)\n",
    "\n",
    "# Instantiate the estimators\n",
    "lr_new = LinearRegression()\n",
    "mlp_new = MLPRegressor(random_state=seed)\n",
    "\n",
    "# Fit the estimators\n",
    "lr_new.fit(X_train_new, y_train_new)\n",
    "mlp_new.fit(X_train_new, y_train_new)\n",
    "\n",
    "# Generate predictions\n",
    "m1_pred = lr_new.predict(X_test_new)\n",
    "m2_pred = mlp_new.predict(X_test_new)\n",
    "\n",
    "# Calculate the RMSE\n",
    "m1_score = np.sqrt(mean_squared_error(y_test_new, m1_pred))\n",
    "m2_score = np.sqrt(mean_squared_error(y_test_new, m2_pred))\n",
    "\n",
    "# Calculate the correlation coefficient\n",
    "m1_corr = np.corrcoef(y_test_new, m1_pred)[0][1]\n",
    "m2_corr = np.corrcoef(y_test_new, m2_pred)[0][1]\n",
    "\n",
    "# Print the result\n",
    "print('Linear regresssion new features')\n",
    "print('  RMSE: %.3f' % (m1_score))\n",
    "print('  correlation: %.3f' % (m1_corr))\n",
    "print('Multi layer perceptron new features')\n",
    "print('  RMSE: %.3f' % (m2_score))\n",
    "print('  correlation: %.3f' % (m2_corr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear regression model achieved a poorer RMSE of 0.025 (previously 0.022) and correlation coefficient of 0.846 (previously 0.888). However, the multi layer perceptron model achieved improved performance with a RMSE of 0.026 (previously on the scaled dataset 0.034) and a correlation coefficient of 0845 (previously 0.69) with the new features.\n",
    "\n",
    "It is up to you to decide on how long you want to spend coming up with and experiment with new features. The amount of time you spend on feature engineering is often based on your desired model performance and project requirements (e.g. deadlines)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your turn\n",
    "\n",
    "Apply the steps used in Notebook: \"Part II Classification\", subsections for cross-validation, generating learning curves, parameter tuning and reporting on your regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are there any ways you can improve the performance? Can you come up with better features or try other [regression algorithms](http://scikit-learn.org/stable/supervised_learning.html#supervised-learning) for this dataset?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
