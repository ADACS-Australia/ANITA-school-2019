{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANITA astroinformatics summer school 2019 - \"Rise of the machines\"\n",
    "\n",
    "## Part II - Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides an introduction to machine learning and walks through how to develop workflows for training machine learning models.\n",
    "\n",
    "This lesson is prepared by:\n",
    "- [Kevin Chai](http://computation.curtin.edu.au/about/computational-specialists/health-sciences/)\n",
    "- [Rebecca Lange](http://computation.curtin.edu.au/about/computational-specialists/humanities/)\n",
    "\n",
    "from the [Curtin Institute for Computation](http://computation.curtin.edu.au) at Curtin University in Perth, Australia. \n",
    "\n",
    "Some of the materials in this notebook have been referenced and adapted from:\n",
    "- [Randal Olsen's Data Science Notebook](https://github.com/rhiever/Data-Analysis-and-Machine-Learning-Projects/tree/master/example-data-science-notebook)\n",
    "- [Sebastian Raschka's Python Machine Learning Notebooks](https://github.com/rasbt/python-machine-learning-book)\n",
    "- [Kevin Markham's Scikit Learn Notebooks](https://github.com/justmarkham/scikit-learn-videos)\n",
    "\n",
    "Make sure to open this notebook in the root directory of the code repository.\n",
    "\n",
    "This work is made available under the [Creative Commons Attribution 4.0 International License](http://creativecommons.org/licenses/by/4.0/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Introduction](#1.-Introduction)\n",
    "2. [Problem definition](#2.-Problem definition)\n",
    "3. [Data preparation](#3.-Data-preparation)\n",
    "4. [Classification](#4.-Classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[ go back to the top ]](#Table-of-contents)\n",
    "\n",
    "This notebook uses several Python packages that come standard with the [Anaconda Python distribution](http://continuum.io/downloads). The primary libraries that we'll be using are:\n",
    "\n",
    "* **NumPy**: a fast numerical array structure and helper functions.\n",
    "* **pandas**: a DataFrame structure to store data in memory and work with it easily and efficiently.\n",
    "* **scikit-learn**: a machine learning package.\n",
    "* **matplotlib**: a basic plotting library; most other plotting libraries are built on top of it.\n",
    "* **seaborn**: a advanced statistical plotting library.\n",
    "\n",
    "To make sure you have all of the packages you need, install them with `conda`:\n",
    "\n",
    "    conda install numpy pandas scikit-learn matplotlib seaborn\n",
    "\n",
    "`conda` may ask you to update some of the packages if you don't have the most recent version. Allow it to do so.\n",
    "\n",
    "Alternatively, if you can install the packages with [pip](https://pip.pypa.io/en/stable/installing/) (a Python package manager):\n",
    "\n",
    "    pip install numpy pandas scikit-learn matplotlib seaborn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[ go back to the top ]](#Table-of-contents)\n",
    "\n",
    "Machine learning refers to algorithms that learn from examples and experiences (data). These algorithms often perform better than explicitly hard coded rules for complex tasks. i.e. where it is difficult to describe precise rules.\n",
    "\n",
    "We will explore two types of machine learning algorithms in this notebook:\n",
    "\n",
    "**1. Supervised Learning** \n",
    "* make predictions using data\n",
    "* There is an outcome we are trying to predict\n",
    "* Example: Is an e-mail spam or ham?\n",
    "\n",
    "![Spam filter](../media/spam_filter.png)\n",
    "<div style=\"text-align:center;font-weight:bold\">Figure: Classification example</div>\n",
    "\n",
    "The workflow for developing and deploying a supervised machine learning model is shown below.\n",
    "\n",
    "![Supervised Learning Workflow](../media/supervised_learning_workflow.jpg)\n",
    "<div style=\"text-align:center;font-weight:bold\">Figure: Supervised learning workflow</div>\n",
    "\n",
    "**2. Unsupervised Learning**\n",
    "* Extract structure from data\n",
    "* There is no \"right answer\"\n",
    "* Example: Segment grocery store shoppers into clusters that exhibit similar behaviours\n",
    "\n",
    "![Clustering](../media/clustering.png)\n",
    "<div style=\"text-align:center;font-weight:bold\">Figure: Clustering example</div>\n",
    "\n",
    "The unsupervised learning workflow is illustrated in  the figure below. Observe that no labels are used for training. \n",
    "\n",
    "![Unsupervised Learning Workflow](../media/unsupervised_learning_workflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Problem definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[ go back to the top ]](#Table-of-contents)\n",
    "\n",
    "We have been tasked to develop a machine learning model to classify galaxies by their morphology (appearance) from a dataset containing measurements such as:\n",
    "\n",
    "- redshift\n",
    "- magnitude in _ugriz_ bands\n",
    "- exponential scale radius and ellipticity\n",
    "- de Vaucouleurs scale radius and ellipticity\n",
    "- stellar mass\n",
    "- ...\n",
    "- etc.\n",
    "\n",
    "Galaxies in the dataset have labels taken from [Galaxy Zoo](https://www.galaxyzoo.org/) DR1 - Table 2. Galaxy Zoo is described in Lintott et al. 2008, MNRAS, 389, 1179 and the data release is described in Lintott et al. 2011, 410, 166. \n",
    "\n",
    "We use the final debiased labels to categorise a galaxy as:\n",
    "\n",
    "- spiral\n",
    "- elliptical\n",
    "\n",
    "![Galaxies](../media/spiral_ellipse_galaxies.jpg)\n",
    "\n",
    "<div style=\"text-align:center;font-weight:bold\">Figure: A spiral galaxy (left) and elliptical galaxy (right)</div>\n",
    "\n",
    "Our goal is to train a model that can accurately classify galaxies. We want our model to generalise well. i.e. it can correctly classify unseen galaxies (i.e. galaxies not in our training dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[ go back to the top ]](#Table-of-contents)\n",
    "\n",
    "The first step of developing a machine learning model is to clearly understand and define the problem we want to solve. Here are some useful questions to ask. \n",
    "\n",
    "#### Does the dataset contain labels?\n",
    "\n",
    "Yes, the dataset contains galaxy morphology labels (spiral and elliptical). Therefore, we can use supervised learning methods to utilise the labelled data.\n",
    "\n",
    "#### What is the type of problem?\n",
    "\n",
    "We want to classify galaxies by their morphology so this is a classification problem.\n",
    "\n",
    "#### What metric can be used to evaluate the model?\n",
    "\n",
    "Since we are performing classification, we can use a classification metric such as [accuracy](https://en.wikipedia.org/wiki/Accuracy_and_precision) to evaluate and quantify the performance of our model. Accuracy is the percentage of correctly classified galaxies. As a challenge, we have been asked to develop a model that achieves ≥ 70% accuracy. Is this feasible? How can we find out?\n",
    "\n",
    "#### Can the question actually be answered with the available data?\n",
    "\n",
    "Yes, the dataset contains labels of the galaxy types we want to classify. i.e. `{spiral, elliptical}`. If we were asked to develop a model to detect other types of galaxies then we would need to collect data and labels for these other types.\n",
    "\n",
    "#### Are there problem and application requirements we need to consider?\n",
    "\n",
    "How will the model be used after it has been developed? Are there requirements for the model to be used on specific devices? e.g. personal computers, smart phones, embedded systems and/or servers. For the purpose of this lesson, we can assume that it is fine to train and run the model on our personal computers. However, this may not always be the case and it is something to consider if we are required to deploy our models to systems with limited compute and memory resources (e.g. embedded systems). In these instances, we may be restricted to build small, less complex and fast models at the cost of reduced performance (e.g. accuracy).\n",
    "\n",
    "<hr />\n",
    "\n",
    "**Thinking about and documenting the problem we're working on is an important step to performing effective machine learning (and data analysis in general) that sometimes gets overlooked.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "\n",
    "\n",
    "**See \"Part I - data prep and exploration\" for details on the data cleaning and exploration process.**\n",
    "\n",
    "\n",
    "For this lesson, we will be using data from [Galaxy Zoo DR1](https://www.galaxyzoo.org/) and the Sloan Digital Sky Survey (SDSS) ([using the DR9 SQL search](http://skyserver.sdss.org/dr9/en/tools/search/sql.asp)). \n",
    "\n",
    "The data dictionary for this dataset is presented in Table 1. This dataset is limited to the first 5,000 Galaxy Zoo classified galaxies which have spectra in the SDSS database. The debiased fraction of the votes in elliptical and spiral categories is given, along with columns identifying systems classified as spiral, elliptical or uncertain.\n",
    "\n",
    "<p style=\"text-align:center;font-weight:bold\">Table 1: Data dictionary</p>\n",
    "\n",
    "| Column           | Description                                                            |\n",
    "|:-----------------|:-----------------------------------------------------------------------|\n",
    "| id               | unique SDSS ID composed of [skyVersion, rerun, run, camcol, field, obj]|\n",
    "| ra               | right ascension  (HMS)                                                 |\n",
    "| dec              | declination (DMS)                                                      |\n",
    "| redshift         | redshift                                                               |\n",
    "| mag_u            | magnitude _u_ band                                                     |\n",
    "| mag_g            | magnitude _g_ band                                                     |\n",
    "| mag_r            | magnitude _r_ band                                                     |\n",
    "| mag_i            | magnitude _i_ band                                                     |\n",
    "| mag_z            | magnitude _z_ band                                                     |\n",
    "| deVRad_r         | de Vaucouleurs scale radius fit in _r_ band                            |\n",
    "| deVAB_r          | ellipticity from de Vaucouleurs fit in _r_ band                        |\n",
    "| expRad_r         | exponential scale radius fit in _r_ band                               |\n",
    "| expAB_r          | ellipticity from exponential fit in _r_ band                           |\n",
    "| stellar_mass     | log galaxy mass (in units of solar mass)                               |\n",
    "| votes            | number of Galaxy Zoo annotators                                        |\n",
    "| p_el_debiased    | debiased labelling probability the galaxy is elliptical                |\n",
    "| p_cs_debiased    | debiased labelling probability the galaxy is spiral                    |\n",
    "| spiral           | label = spiral galaxy {0=False, 1=True}                                |\n",
    "| elliptical       | label = elliptical galaxy {0=False, 1=True}                            |\n",
    "| uncertain        | label = uncertain {0=False, 1=True}                                    |\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the clean dataset\n",
    "df = pd.read_csv('data/galaxies-clean.csv')\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The summary table provides some useful information:\n",
    "\n",
    "- there are 1,797 galaxies in our dataset\n",
    "- the mean of the `class` column indicates that ~75.3% of the dataset are spiral galaxies\n",
    "- min values for the radius columns range from 0.31 to 0.38\n",
    "- min value for the `stellar_mass` = 8.096\n",
    "- min value for the `mag` ranging from 15.59 to 18.5\n",
    "\n",
    "Our cleaned dataset contains 1,797 galaxies and 75.3% of the records are spiral galaxies.\n",
    "\n",
    "---\n",
    "\n",
    "For completeness, let's generate a scatterplot for all features in one figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Command to show plots inside of the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "\n",
    "magnitudes = ['mag_u', 'mag_g', 'mag_r', 'mag_i', 'mag_z']\n",
    "measurements = ['redshift', 'stellar_mass', 'deVRad_r', 'deVAB_r', 'expRad_r', 'expAB_r']\n",
    "\n",
    "\n",
    "# Plot scatterplot matrix with all features\n",
    "selected_columns = measurements + magnitudes + ['class']\n",
    "sb.pairplot(df[selected_columns], hue='class', plot_kws={'alpha': 0.3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[ go back to the top ]](#Table-of-contents)\n",
    "\n",
    "Cleaning and exploring the data is a important component to any machine learning project. If we had jumped straight in to modelling, we would have trained a model with errors in the dataset. **Bad data leads to bad models.**\n",
    "\n",
    "Now it's a good time to introduce some machine learning terminology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terminology\n",
    "\n",
    "- Each row is an `example` (also known as (aka): observation, sample, instance, record)\n",
    "- Each column is a `feature` (aka: predictor, attribute, dimension, independent variable)\n",
    "- The value we are predicting is the `label` (aka: target, outcome, dependent variable) \n",
    "\n",
    "In classification, the label is also referred to as the class or category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Notation\n",
    "\n",
    "Let:\n",
    "- `m` = the number of examples in our dataset\n",
    "- `n` = the number of features in our dataset\n",
    "\n",
    "The dataset can be represented by two variables as shown in Table 2:\n",
    "\n",
    "1. A matrix `X` containing the examples and features of size `m x n`\n",
    "2. A vector `y` containing the labels of size `m`\n",
    "\n",
    "<p style=\"text-align:center;font-weight:bold\">Table 2: Dataset notations</p>\n",
    "\n",
    "<table>\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th style=\"text-align:center\" colspan=3>X</th>\n",
    "            <th style=\"text-align:center\">y</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th style=\"text-align:center\">feature 1 ($x_1$)</th>\n",
    "            <th style=\"text-align:center\">feature 2 ($x_2$)</th>\n",
    "            <th style=\"text-align:center\">feature 3 ($x_3$)</th>\n",
    "            <th style=\"text-align:center\">label</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td style=\"text-align:center\">5</td>\n",
    "            <td style=\"text-align:center\">12</td>\n",
    "            <td style=\"text-align:center\">3</td>\n",
    "            <td style=\"text-align:center\">0</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align:center\">1</td>\n",
    "            <td style=\"text-align:center\">3</td>\n",
    "            <td style=\"text-align:center\">8</td>\n",
    "            <td style=\"text-align:center\">1</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align:center\">9</td>\n",
    "            <td style=\"text-align:center\">3</td>\n",
    "            <td style=\"text-align:center\">2</td>\n",
    "            <td style=\"text-align:center\">1</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align:center\">5</td>\n",
    "            <td style=\"text-align:center\">8</td>\n",
    "            <td style=\"text-align:center\">7</td>\n",
    "            <td style=\"text-align:center\">0</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align:center\">2</td>\n",
    "            <td style=\"text-align:center\">7</td>\n",
    "            <td style=\"text-align:center\">8</td>\n",
    "            <td style=\"text-align:center\">0</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "This is the standard notation used in machine learning. Following this convention makes it easier for other machine learning practitioners to understand your scripts. This notation also allows us to concisely define supervised machine learning and classification described in the [Introduction](#1.-Introduction) section.\n",
    "\n",
    "#### Supervised learning\n",
    "Learn a function `f` that maps features to labels\n",
    "<p style=\"text-align:center;font-weight:bold\">$f(X) \\rightarrow y$</p>\n",
    "\n",
    "\\begin{align}\n",
    "y &= \\begin{bmatrix}\n",
    "    y_1 \\\\\n",
    "    \\vdots \\\\\n",
    "    y_i \\\\\n",
    "    \\vdots \\\\\n",
    "    y_{m}\n",
    "    \\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "With classification, $y_i = \\{c_1, ..., c_k\\},where\\ k = number\\ of\\ classes$\n",
    "\n",
    "We only have two classes in our dataset so $y_i \\in \\{0=elliptical, 1=spiral\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit Learn\n",
    "We will be using the [scikit-learn](http://scikit-learn.org/) machine learning library for developing our models. As such, we need to prepare our dataset into a format that scikit-learn expects:\n",
    "\n",
    "1. Features and labels are **separate objects**\n",
    "2. Features and labels should be **numeric**\n",
    "3. Features and labels should be **`numpy` arrays**\n",
    "4. Features and labels should have **specific shapes**\n",
    "\n",
    "**Note**: `pandas` is built on top of `numpy` so we can create the features matrix as a pandas `DataFrame` and labels vector as a pandas `Series`.\n",
    "\n",
    "Let's prepare the dataset for scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Set a random seed number to repoduce our results\n",
    "seed = 19\n",
    "\n",
    "# Load the clean dataset\n",
    "df = pd.read_csv('data/galaxies-clean.csv')\n",
    "\n",
    "# Selected columns for modelling\n",
    "measurements = \n",
    "magnitudes = \n",
    "features = \n",
    "\n",
    "# features matrix\n",
    "X = \n",
    "\n",
    "# labels vector\n",
    "y = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify our features matrix and labels vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features matrix\n",
    "# expected output: pandas DataFrame, (number of rows, number of features)\n",
    "print('X: %s, %s' % (type(X), X.shape))\n",
    "\n",
    "# Labels vector \n",
    "# expected output: pandas Series, (number of rows,)\n",
    "print('y: %s, %s' % (type(y), y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks great. Now we can start modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[ go back to the top ]](#Table-of-contents)\n",
    "\n",
    "### Splitting the data\n",
    "\n",
    "It's time to make the next big step in our analysis: splitting the data into training and test sets.\n",
    "\n",
    "- A **training set** is a random subset of the data that we use to train our models.\n",
    "- A **test set** is a random subset of the data (mutually exclusive from the training set) that we use to test our models.\n",
    "\n",
    "In machine learning we are always concerned that our models will **overfit** the data. i.e. The model learns the training set so well that it won't be able to handle examples it's never seen before. This is why it's important for us to build the model with the training set, but score it with a separate unseen testing set.\n",
    "\n",
    "Once we split the data into a training and test set, we should treat the test set like it no longer exists. We cannot use any information from the testing set to build our model or else we're cheating. The training dataset can also be split again to create a validation set to tune our models. It is important that the validation set is separate from the holdout test dataset.\n",
    "\n",
    "This process is illustrated in the figure below.\n",
    "\n",
    "<div style=\"text-align:center;font-weight:bold\">Figure: Holdout set method to split the dataset</div>\n",
    "<img src=\"../media/holdout.png\" />\n",
    "\n",
    "\n",
    "\n",
    "Now let's split the dataset using the `train_test_split` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the train_test_split function\n",
    "\n",
    "# Split the dataset into 80% training, 20% testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the dataset split, we can start fitting models to our data. We have heard from our colleagues that they have had success using decision tree classifiers in their projects so let's start with those.\n",
    "\n",
    "**Decision tree** classifiers are incredibly simple in theory. In their simplest form, decision tree classifiers ask a series of Yes/No questions about the data — each time getting closer to finding out the class of each example — until they either classify the data set perfectly or simply can't differentiate a set of examples.\n",
    "\n",
    "An example decision tree classifier for approving loan applications is shown in the following figure.\n",
    "\n",
    "<div style=\"text-align:center;font-weight:bold\">Figure: Decision tree example</div>\n",
    "<img src=\"../media/decision_tree_example.png\" />\n",
    "\n",
    "Notice how the classifier asks yes/no questions about the data. e.g. whether the applicant owns a house so it can differentiate the records. \n",
    "\n",
    "**check the slides for a walk-through of how the decision tree works**\n",
    "\n",
    "Decision tree classifiers are *scale-invariant*, i.e. the scale of the features does not affect their performance unlike many machine learning models. In other words, it doesn't matter if our features range from 0 to 1 or 0 to 1,000; decision tree classifiers will work with them just the same.\n",
    "\n",
    "There are several [parameters](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) that we can tune for decision tree classifiers and different [metrics](https://en.wikipedia.org/wiki/Evaluation_of_binary_classifiers) we can use to evaluate its performance. For now let's use a basic decision tree and the accuracy performance metric.\n",
    "\n",
    "\\begin{align}\n",
    "accuracy &= \\dfrac{correct\\ classifications}{total\\ number\\ of\\ classifications}\n",
    "\\end{align}\n",
    "\n",
    "Scikit learn provides a 4 step modelling pattern which makes it easy to switch in different models / algorithms for your dataset. This pattern is described in the code and comments below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import the model\n",
    "\n",
    "# Step 2: Instantiate the model\n",
    "\n",
    "# Step 3: Fit the model on data (i.e. train the model)\n",
    "\n",
    "# Step 4: Generate predictions / scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's great! Our model achieves 91% accuracy without much effort. We have already beaten the challenge to build a model with ≥ 70% accuracy.\n",
    "\n",
    "However, there's a catch. Depending on how our training and testing set is sampled, our model can achieve anywhere from 87% to 96% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_accuracies = []\n",
    "\n",
    "# Split the dataset differently and fit a model on this split, 1,000 times\n",
    "for i in range(1000):\n",
    "    X2_train, X2_test, y2_train, y2_test = train_test_split(X, y, train_size=0.8, random_state=i)\n",
    "    \n",
    "    decision_tree_classifier = DecisionTreeClassifier(random_state=seed)\n",
    "    decision_tree_classifier.fit(X2_train, y2_train)\n",
    "    classifier_accuracy = decision_tree_classifier.score(X2_test, y2_test)\n",
    "    model_accuracies.append(classifier_accuracy)\n",
    "\n",
    "sb.distplot(model_accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a problem! The model performance varies a lot depending on the subset of the data it's trained on. This means the model is **overfitting**: the model learns to classify the training set so well that it doesn't generalize and perform well on data it hasn't seen before (i.e. the test set)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "\n",
    "Cross validation is a method of splitting the dataset to help address this problem. For this lesson, we will use the ***k*-fold cross-validation** method. This involves splitting the original data set into *k* subsets, use one of the subsets as the testing set and, the rest of the subsets are used as the training set. This process is repeated *k* times such that each subset is used as the testing set exactly once. This process is illustrated in Figure X.\n",
    "\n",
    "<div style=\"text-align:center;font-weight:bold\">Figure: k-fold cross-validation</div>\n",
    "<img src=\"../media/k-fold.png\" />\n",
    "\n",
    "10-fold cross-validation is the most common choice so let's use that here. Performing 10-fold cross-validation on our data set looks something like this on a subset of 100 examples in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cv(cv, n_samples):\n",
    "    masks = []\n",
    "    for train, test in cv:\n",
    "        mask = np.zeros(n_samples, dtype=bool)\n",
    "        mask[test] = 1\n",
    "        masks.append(mask)\n",
    "        \n",
    "    plt.figure(figsize=(15, 15))\n",
    "    plt.imshow(masks, interpolation='none')\n",
    "    plt.ylabel('Fold')\n",
    "    plt.xlabel('Row #')\n",
    "    \n",
    "# Import the function\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "\n",
    "# Use a subset of 100 examples\n",
    "subset = y[0:100]\n",
    "\n",
    "# Plot the data split\n",
    "plot_cv(StratifiedKFold(subset, n_folds=10), len(subset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each square in the plot represents an example in our dataset.\n",
    "\n",
    "You'll notice that we used the `StratifiedKFold` function in the code above. Stratified means we keep the class percentage the same across all of the folds (~75% spiral), which is important for maintaining a representative subset of our dataset. i.e. we don't want to end up having 100% spiral galaxies in one of the folds.\n",
    "\n",
    "We can fit a decision tree classifier using 10-fold cross-validation with the `cross_val_score` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the cross_val_score function\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Create a decision tree classifier\n",
    "dtc = \n",
    "\n",
    "# cross_val_score returns a list of the scores, which we can visualize\n",
    "# to get a reasonable estimate of our classifier's performance\n",
    "cv_scores = \n",
    "\n",
    "# Plot the results\n",
    "sb.distplot(cv_scores)\n",
    "plt.title('Average score: {}'.format(np.mean(cv_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks much better. We have a more consistent score (less variance) of our model's classification accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[ go back to the top ]](#Table-of-contents)\n",
    "\n",
    "Learning curves allow us to evaluate the performance of our models as they are provided more training examples. i.e. They show us how or if our model learns. Plotting these curves also allows us to diagnose our models in order to gain insights on how we can improve them. More specifically, learning curves allows us to identify:\n",
    "\n",
    "- underfitting (high bias): model performs poorly on training and validation\n",
    "    - Try more features\n",
    "    - Decrease regularisation\n",
    "- overfitting (high variance): model performance is good on training but much poorer on validation\n",
    "    - Get more data\n",
    "    - Use less features\n",
    "    - Increase regularisation\n",
    "\n",
    "Ideally, we want to develop a model that achieves similar performance on both the training and validation sets (good bias and variance trade off) as shown in the figure below.\n",
    "\n",
    "<div style=\"text-align:center;font-weight:bold\">Figure: Learning curves</div>\n",
    "<img src=\"../media/learning_curves.png\" />\n",
    "\n",
    "_Source: Andrew Ng, Coursera - Machine Learning. 2012_\n",
    "\n",
    "Let's plot the learning curve for our decision tree classifier using the `learning_curve` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the learning_curve function\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "# Create a decision tree classifier\n",
    "dtc = \n",
    "# Generate the learning curve scores \n",
    "train_sizes, train_scores, val_scores = \n",
    "\n",
    "# Calculate the result averages and standard deviatio\n",
    "train_mean = \n",
    "train_std = \n",
    "val_mean = \n",
    "val_std = \n",
    "\n",
    "# Plot the training learning curve\n",
    "plt.plot(train_sizes, train_mean,\n",
    "         color='green', marker='o',\n",
    "         markersize=5, label='training accuracy')\n",
    "\n",
    "plt.fill_between(train_sizes,\n",
    "                 train_mean + train_std,\n",
    "                 train_mean - train_std,\n",
    "                 alpha=0.15, color='green')\n",
    "\n",
    "# Plot the validation learning curve\n",
    "plt.plot(train_sizes, val_mean,\n",
    "         color='blue', linestyle='--',\n",
    "         marker='s', markersize=5,\n",
    "         label='validation accuracy')\n",
    "\n",
    "plt.fill_between(train_sizes,\n",
    "                 val_mean + val_std,\n",
    "                 val_mean - val_std,\n",
    "                 alpha=0.15, color='blue')\n",
    "\n",
    "# Plot settings\n",
    "plt.xlabel('Number of training samples')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylim([0.8, 1])\n",
    "plt.tight_layout()\n",
    "# plt.savefig('images/learning_curve.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the learning curve we can see that the decision tree classifier overfits the training dataset. The training accuracy remains constant at 100% (fits the data perfectly) while the average validation scores steadily increase from 87% to 93% as the number of training samples increases.\n",
    "\n",
    "Now that we know our decision tree classifier overfits the dataset, let's do something about it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter tuning\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "\n",
    "Every machine learning model comes with a variety of parameters to tune, and these parameters can be vitally important to the performance of our classifier. For example, using the default parameters for decision trees has resulted in our classifier overfitting the training dataset. \n",
    "\n",
    "As an extreme example, let's limit the depth of our decision tree classifier to a `max_depth = 1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the max_depth of the decision tree to 1\n",
    "dtc = \n",
    "\n",
    "# Calculate the cross validation score\n",
    "cv_scores =\n",
    "\n",
    "# Plot the results\n",
    "sb.distplot(cv_scores, kde=False)\n",
    "plt.title('Average score: {}'.format(np.mean(cv_scores)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average classification accuracy has dropped to 86%.\n",
    "\n",
    "We need to use a systematic method to discover the best parameters for our model and dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid search\n",
    "\n",
    "A common method for model parameter (hyperparameter) tuning is grid search. The idea behind grid search is to explore a range of parameters and find the best-performing parameter combination. Focus your search on the best range of parameters, then repeat this process several times until the best parameters are discovered.\n",
    "\n",
    "Let's tune our decision tree classifier using the `GridSearchCV` function. We'll stick to only two parameters for now (`max_depth` and `max_features`), but it's possible to simultaneously explore dozens of parameters if we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the GridSearchCV function\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create a decision tree classifier\n",
    "dtc = \n",
    "\n",
    "# Parameter values to explore\n",
    "parameter_grid =\n",
    "\n",
    "# Create a cross validation object using StratifiedKFold\n",
    "cross_validation = \n",
    "\n",
    "# Instantiate the grid search using GridSearchCV\n",
    "grid_search = \n",
    "\n",
    "# Fit the decision tree using grid search\n",
    "\n",
    "# Display the results\n",
    "print('Best score: {}'.format(grid_search.best_score_))\n",
    "print('Best parameters: {}'.format(grid_search.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize the grid search to see how the parameters interact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_visualization = []\n",
    "\n",
    "for grid_pair in grid_search.grid_scores_:\n",
    "    grid_visualization.append(grid_pair.mean_validation_score)\n",
    "\n",
    "grid_visualization = np.array(grid_visualization)\n",
    "grid_visualization.shape = (6, 5)\n",
    "sb.heatmap(grid_visualization, cmap='Blues')\n",
    "plt.xticks(np.arange(5) + 0.5, grid_search.param_grid['max_features'])\n",
    "plt.yticks(np.arange(6) + 0.5, grid_search.param_grid['max_depth'][::1])\n",
    "plt.xlabel('max_features')\n",
    "plt.ylabel('max_depth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a better sense of the parameter space: a `max_depth > 4` is needed for the decision tree perform reasonably well. Tuning the `max_features` doesn't seem to make as much difference as we achieved the best performance with using `9` out of the 11 features.\n",
    "\n",
    "An alternative way to visualise the performance of your parameter tuning results is to plot [validation curves](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.validation_curve.html).\n",
    "\n",
    "Let's go ahead and use a broader grid search to find the best settings for more parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a decision tree classifier\n",
    "dtc = \n",
    "\n",
    "# Hyperparameter values to explore\n",
    "parameter_grid =\n",
    "\n",
    "# Create a cross validation object using StratifiedKFold\n",
    "cross_validation = \n",
    "\n",
    "# Instantiate the grid search using GridSearchCV\n",
    "grid_search = \n",
    "\n",
    "# Perform the grid search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Display the best model scores and parameters\n",
    "print('Best score: {}'.format(grid_search.best_score_))\n",
    "print('Best parameters: {}'.format(grid_search.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the best classifier from the grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc = grid_search.best_estimator_\n",
    "# Examine the model\n",
    "dtc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot the learning curve for the best classifier for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the learning curve scores\n",
    "train_sizes, train_scores, val_scores = \n",
    "\n",
    "# Calculate the result averages and standard deviation\n",
    "train_mean = \n",
    "train_std = \n",
    "val_mean = \n",
    "val_std = \n",
    "\n",
    "\n",
    "# Plot the training learning curve\n",
    "plt.plot(train_sizes, train_mean,\n",
    "         color='green', marker='o',\n",
    "         markersize=5, label='training accuracy')\n",
    "\n",
    "plt.fill_between(train_sizes,\n",
    "                 train_mean + train_std,\n",
    "                 train_mean - train_std,\n",
    "                 alpha=0.15, color='green')\n",
    "\n",
    "# Plot the validation learning curve\n",
    "plt.plot(train_sizes, val_mean,\n",
    "         color='blue', linestyle='--',\n",
    "         marker='s', markersize=5,\n",
    "         label='validation accuracy')\n",
    "\n",
    "plt.fill_between(train_sizes,\n",
    "                 val_mean + val_std,\n",
    "                 val_mean - val_std,\n",
    "                 alpha=0.15, color='blue')\n",
    "\n",
    "# Plot settings\n",
    "plt.xlabel('Number of training samples')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylim([0.8, 1])\n",
    "plt.tight_layout()\n",
    "# plt.savefig('images/learning_curve.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This learning curve looks better. The gap between the training and validation accuracy has reduced with training score of 95% and a validation score of 93% which suggests the model is no longer overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reporting\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "\n",
    "Score the classifier on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best estimator from the grid search\n",
    "dtc=\n",
    "\n",
    "# Score the estimator on the test dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our best classifier achieves a test accuracy score of 92.7% which is almost on par with our validation score of 93%. This gives us confidence that our experiment design and trained model is providing realistic results.\n",
    "\n",
    "We can generate a confusion matrix using the `confusion_matrix` function to get more detailed classification results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the confusion_matrix function\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Generate the predictions\n",
    "y_pred = dtc.predict(X_test)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Create a DataFrame for plotting\n",
    "labels = ['elliptical', 'spiral']\n",
    "df_cm = pd.DataFrame(cm, columns=labels, index=labels)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "sb.heatmap(df_cm, annot=True, fmt='g', cmap='Blues', annot_kws={\"size\": 15})\n",
    "plt.title('Confusion Matrix', fontsize=20)\n",
    "plt.xlabel('Actual', fontsize=15)\n",
    "plt.xticks(fontsize=13)\n",
    "plt.ylabel('Predicted', fontsize=15)\n",
    "plt.yticks(fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix provides more detailed information of where the classifier made mistakes (i.e. where it was confused). Here we can see that the model predicted 12 galaxies as spiral that were actually elliptical and 14 elliptical galaxies that were actually spiral. Overall, it looks like the decision tree classifier has performed quite well on test dataset.\n",
    "\n",
    "We can also visualize the decision tree with [GraphViz](http://www.graphviz.org/) to see how it's making the classifications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.tree as tree\n",
    "from sklearn.externals.six import StringIO\n",
    "\n",
    "with open('../media/galaxy_tree.dot', 'w') as out_file:\n",
    "    out_file = tree.export_graphviz(decision_tree_classifier, out_file=out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The exported decision tree graph displays features by their index. e.g. X[0], ... X[10]\n",
    "# This snippet will replace the index names with the actual feature names\n",
    "replacements = {}\n",
    "for index, feature in enumerate(features):\n",
    "    key = 'X[%d]' % (index)\n",
    "    replacements[key] = feature\n",
    "\n",
    "with open('../media/galaxy_tree.dot') as infile, open('../media/galaxy_tree_names.dot', 'w') as outfile:\n",
    "    for line in infile:\n",
    "        for src, target in replacements.items():\n",
    "            line = line.replace(src, target)\n",
    "        outfile.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have [GraphViz](http://www.graphviz.org/) installed then you can generate a image of the decision tree with the following command:\n",
    "\n",
    "    dot -Tpng images/galaxy_tree_names.dot -o images/galaxy_tree_names.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../media/galaxy_tree_names.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright! We finally have our demo classifier. Let's create some more visuals of its performance so we have something to put in our report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_scores = cross_val_score(decision_tree_classifier, X, y, cv=10)\n",
    "sb.boxplot(dt_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot is not very useful with just one model so let's train another.\n",
    "\n",
    "A common problem with decision trees is that they're prone to overfitting. They can add complex rules to the point that they classify the training set near-perfectly (as we saw early with our learning curves), but fail to generalise well to data they have not seen before.\n",
    "\n",
    "\n",
    "# TO DO: include Random forest image from slides\n",
    "\n",
    "**Random Forest classifiers** work around this limitation by creating many decision trees (i.e. a forest), each trained on random subsets of training samples and features. These decision trees are then combined to make a more accurate classification. Let's see if a random forest classifier works better for our datatset. \n",
    "\n",
    "The great part about scikit-learn is that the modelling pattern of training, testing, parameter tuning, etc. process is the same for all models. We only need to plug in the new classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import the class\n",
    "\n",
    "# Step 2: Instantiate the estimator\n",
    "random_forest_classifier = \n",
    "\n",
    "# Hyperparameter values to explore\n",
    "parameter_grid = \n",
    "\n",
    "# Create a cross validation object using StratifiedKFold\n",
    "cross_validation =\n",
    "\n",
    "# Instantiate the grid search using GridSearchCV \n",
    "grid_search = \n",
    "\n",
    "# Step 3 & 4: Fit the estimator on data (i.e. train the model) and generate predictions\n",
    "\n",
    "# Display the best model scores and parameters\n",
    "print('Best score: {}'.format(grid_search.best_score_))\n",
    "print('Best parameters: {}'.format(grid_search.best_params_))\n",
    "\n",
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest classifier achieved a 95% cross validation accuracy score. Find out how well it performs on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best estimator from the grid search\n",
    "random_forest_classifier = \n",
    "\n",
    "# Score the estimator on the test dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forests performs better than the decision tree classifier on the test dataset:\n",
    "\n",
    "- decision trees ~ 92%\n",
    "- random forests ~ 96%\n",
    "\n",
    "We can also plot and compare their cross validation performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_df = pd.DataFrame({'accuracy': cross_val_score(random_forest_classifier, X_train, y_train, cv=10),\n",
    "                       'classifier': ['Random Forest'] * 10})\n",
    "dt_df = pd.DataFrame({'accuracy': cross_val_score(decision_tree_classifier, X_train, y_train, cv=10),\n",
    "                      'classifier': ['Decision Tree'] * 10})\n",
    "both_df = rf_df.append(dt_df)\n",
    "\n",
    "sb.boxplot(x='classifier', y='accuracy', data=both_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forests also performs better on average than decisions trees with cross validation. Random forest classifiers can perform particularly well when there's hundreds of possible features to model (we only have 11 features in dataset). In other words, there was not much room for improvement for random forests over decision trees with this dataset.\n",
    "\n",
    "To report more detailed information of our model, let's see what features were considered the most important for both classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision tree features\n",
    "sorted(zip(dtc.feature_importances_, features), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forests features\n",
    "sorted(zip(random_forest_classifier.feature_importances_, features), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both classifiers rank the importance of the features in a similar order with the exception of the magnitudes. The stellar mass, ellipticity, radius and redshift of the galaxies were the most important features. \n",
    "\n",
    "If you want to explore this further you could experiment with removing the `mag_u`, `mag_i`, `mag_g` features as they scored a feature importance of `0` for the decision tree classifier to see how it affects classification performance. Additionally, you can experiment with various [feature selection methods](http://scikit-learn.org/stable/modules/feature_selection.html) for systemtically selecting a reduced feature set.\n",
    "\n",
    "**see the slides for an explanation of the limitations of the tree classifier**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many [classification algorithms](http://scikit-learn.org/stable/supervised_learning.html#supervised-learning) we can experiment with on our dataset. Try fitting a [Logistic Regression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html), a [Linear Support Vector Classifier (SVM)](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC), or a [Gradient Boosting Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) model on the dataset. It is up to you whether you want to perform parameter tuning on these models. \n",
    "\n",
    "Generate boxplots so we can compare the performance with the decision tree and random forest models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required models\n",
    "# Import the required models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Initialise the models\n",
    "\n",
    "# Calculate the cross validation scores\n",
    "\n",
    "# Create a DataFrame of the results\n",
    "\n",
    "# Merge all classifier results\n",
    "classifiers_df = rf_df.append([dt_df, # , # ])\n",
    "\n",
    "# Generate a box plot comparing the different algorithms\n",
    "sb.boxplot(x='classifier', y='accuracy', data=classifiers_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your turn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick your favourite model and use it to predict the morphology of galaxies that have been labelled as  uncertain in the dataset. \n",
    "\n",
    "- How many spiral vs. elliptical galaxies does the model predict?\n",
    "- How do the model predictions compare with the debiased labelled probabilities?\n",
    "- How would you use this model and results in practice? Remember that our original data included many uncertain galaxies, try to predict their type. How do you decide which prediction to use/trust? Can you improve your model for future predictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
